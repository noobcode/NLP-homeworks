{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 2.1: Markov Models. Hidden Markov Models and Part of Speech Tagging.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Create a tri-gram model for generating pseudo-Trump sentences \n",
    "- load the corpus, tokenize it and obtain list of trigrams \n",
    "- define a function that obtains the counts of the \"model\" \n",
    "- define a function that generates a pseudo-sentence \n",
    "- when generating a sentence, make sure that your sentence fulfils the following requirements\n",
    "    - it is at least 5 words long\n",
    "    - the last token of the pseudo-sentence is a \".\", \"!\", or \"?\"\n",
    "    - it does not contain any other \".\", \"!\", \"?\" tokens other than the final one\n",
    "- print 5 pseudo-sentences\n",
    "\n",
    "2) Use the built-in n-gram HMM models in nltk to tag a corpus \n",
    "- load the brown corpus\n",
    "- split each category in the corpus to test and train\n",
    "- for each category in the corpus, train on the train set and evaluate on the test set the following taggers:\n",
    "    - default\n",
    "    - affix\n",
    "    - unigram\n",
    "    - bigram\n",
    "    - trigram\n",
    "    \n",
    "    Each tagger should have backoff configured on the previous tagger.\n",
    "    \n",
    "    Print the results in a table.\n",
    "    \n",
    "    \n",
    "- repeat the previous experiment using universal tagset. Print the results in a table.\n",
    "- cross evaluate between different genres (train on one category, evaluate on all the other categories). Print and compare the results\n",
    "- Only for the \"news\" portion of the corpus, compare\n",
    "    - the best berforming tagger (with backoff)\n",
    "    - the naive bayes tagger\n",
    "    \n",
    "    Compare the accuracy as well as the execution time.\n",
    "    \n",
    "    Use both the universal tagset and the full tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import codecs\n",
    "import codecs\n",
    "\n",
    "# Import taggers\n",
    "from nltk import DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk import ClassifierBasedPOSTagger\n",
    "\n",
    "# Import the brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I call him , he ’ s so big , beautiful bikes ... I call him and you see very easily I think it ’ s really .\n",
      "THE PRESS , I think based on what Simon and Schuster agreed with me in Iowa and we ’ re not going to .\n",
      "I hated some of the other .\n",
      "I wanted to endorse .\n",
      "I 'm not getting – anything much for .\n"
     ]
    }
   ],
   "source": [
    "# Homework 2 part 1\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "\n",
    "import random\n",
    "\n",
    "class HMM(object):\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.db = {}\n",
    "        self.tokens = tokens\n",
    "        self.count = len(self.tokens)\n",
    "        self.createdb()\n",
    "    # train the model by couting bigram following words\n",
    "    def createdb(self):\n",
    "        trigrams = nltk.trigrams(self.tokens)\n",
    "        for w1, w2, predicted in trigrams:\n",
    "            key = (w1, w2)\n",
    "            if key in self.db:\n",
    "                self.db[key].append(predicted)\n",
    "            else:\n",
    "                self.db[key] = [predicted]\n",
    "    # generate a sentence ending with \".!?\" starting with a random bigram\n",
    "    def generate_sentence(self):\n",
    "        rng = random.randint(0, self.count-3)\n",
    "        # Start with uppercase\n",
    "        while not self.tokens[rng].isupper():\n",
    "            rng = (rng + 1) % self.count-3\n",
    "        first, second = self.tokens[rng], self.tokens[rng+1]\n",
    "        generated = []\n",
    "        for i in range(50):\n",
    "            generated.append(first)\n",
    "            first, second = second, random.choice(self.db[(first, second)])\n",
    "            if second in \".!?\":\n",
    "                break        \n",
    "        generated.append(second)\n",
    "        return generated\n",
    "    # generate x sentences of length at least 5\n",
    "    def printXSentences(self, count):\n",
    "        senCount = 0\n",
    "        while (senCount < count):\n",
    "            sentence = self.generate_sentence()\n",
    "            if len(sentence) > 4:           \n",
    "                print(' '.join(sentence))\n",
    "                senCount = senCount + 1\n",
    "\n",
    "def hw2_part1():\n",
    "    \n",
    "    # Trump speeches file location\n",
    "    fname = \"speeches.txt\"\n",
    "    # Read the corpus\n",
    "    raw_corpus = codecs.open(fname,'r','utf8').read()\n",
    "    \n",
    "    # Tokenize the corpus\n",
    "    corpus = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "    # Generate list of trigrams\n",
    "    # Initialize the \"markov model\"\n",
    "    # Preferably, you should define a function (or an object)\n",
    "    # which \"trains\" the model. You should just invoke the function here.\n",
    "    # Fill in all the counts \n",
    "\n",
    "    model = HMM(corpus)\n",
    "    \n",
    "    # Generate a sentence\n",
    "    # Preferably you should define a function (or an object)\n",
    "    # which \"generates\" a sentence following the requirements (min length, ending with punctuation, etc)\n",
    "    # You should just invoke the code here\n",
    "    \n",
    "            \n",
    "    # Print the sentences\n",
    "    model.printXSentences(5)\n",
    "    \n",
    "hw2_part1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-3-ca7336e35ce6>, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ca7336e35ce6>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    print \"############## BROWN TAGSET ##################\\n\"\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# Homework 2 part 2\n",
    "\n",
    "# Function that splits a corpus in train and test\n",
    "def split_train_test(corpus,test_size=500):\n",
    "    return corpus[test_size:], corpus[:test_size]\n",
    "\n",
    "def split_each_category_in_train_and_test(corpus, fraction_ts, tagset=None):\n",
    "    train_sets = {} # dictionary of training sets, one key for each category\n",
    "    test_sets = {}  # dictionary of test sets, one key for each category\n",
    "    for category in corpus.categories():\n",
    "        corpus_tsents = corpus.tagged_sents(categories=category, tagset=tagset)\n",
    "        tsents_train, tsents_test = split_train_test(corpus_tsents, test_size=int(fraction_ts * len(corpus_tsents)))\n",
    "        # add training and test set to the corresponding dictionary\n",
    "        train_sets[category] = tsents_train\n",
    "        test_sets[category] = tsents_test\n",
    "        \n",
    "    return train_sets, test_sets\n",
    "\n",
    "def get_most_common_tag(train_set):\n",
    "    flat_list = [item for sublist in train_set for item in sublist]\n",
    "    tags = [tag for (word, tag) in flat_list]\n",
    "    # Get the most frequent tag in the training set\n",
    "    most_frequent_tag = nltk.FreqDist(tags).max()\n",
    "    return most_frequent_tag\n",
    "\n",
    "def cross_evaluate(model, test_sets):\n",
    "        accuracies = {'TS_' + category : round(model.evaluate(test_set),4) * 100 \n",
    "                      for (category, test_set) in test_sets.iteritems()}\n",
    "        return accuracies\n",
    "    \n",
    "def cross_evaluate_all(tagger_name, models_df_uni, test_sets_uni):\n",
    "    model_cross_results = {'TR_' + category : [] for category in brown.categories()}\n",
    "    for category in brown.categories():\n",
    "        tagger_model = models_df_uni.loc[category, tagger_name]\n",
    "        accuracies = cross_evaluate(tagger_model, test_sets_uni)\n",
    "        model_cross_results['TR_'+category] = accuracies\n",
    "    return pd.DataFrame(model_cross_results)\n",
    "\n",
    "# For each category, train and evaluate taggers. Use backoff.\n",
    "def train_and_evaluate_taggers(taggers_list, corpus, train_sets, test_sets):\n",
    "    # dictionary to keep the results of each tagger on each category\n",
    "    benchmarks_df = {t.__name__: [] for t in taggers_list}\n",
    "    # dictionary to keep models trained on each category\n",
    "    models_df = {t.__name__: [] for t in taggers_list}\n",
    "    \n",
    "    for i, category in enumerate(brown.categories()):\n",
    "        for tagger in taggers_list:\n",
    "            if tagger == DefaultTagger:\n",
    "                most_common_tag = get_most_common_tag(train_sets[category])\n",
    "                # the default tagger does not have a backoff tagger\n",
    "                tagger_model = tagger(most_common_tag)\n",
    "            else:\n",
    "                # each tagger takes as backoff tagger the previous tagger\n",
    "                tagger_model = tagger(train_sets[category], backoff=tagger_model)\n",
    "            # evaluate tagger\n",
    "            accuracy = round(tagger_model.evaluate(test_sets[category]),4) * 100\n",
    "            # append to dictionaries accuracy and model\n",
    "            benchmarks_df[tagger.__name__].append(accuracy)\n",
    "            models_df[tagger.__name__].append(tagger_model)\n",
    "    \n",
    "    # returns the statistics for all taggers and all categories\n",
    "    benchmarks_df = pd.DataFrame(benchmarks_df, index=brown.categories())\n",
    "    models_df = pd.DataFrame(models_df, index=brown.categories())\n",
    "    return benchmarks_df, models_df\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "def hw2_part2():\n",
    "    # list of taggers to benchmark\n",
    "    taggers = [DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger]\n",
    "    fraction_ts = 0.2 # faction test set\n",
    "    \n",
    "    ### BROWN TAGSET   \n",
    "    # Split each category in the brown corpus into train and test\n",
    "    print \"############## BROWN TAGSET ##################\\n\"\n",
    "    train_sets_full, test_sets_full = split_each_category_in_train_and_test(brown, fraction_ts)\n",
    "    brown_tagset_df, models_df_full = train_and_evaluate_taggers(taggers, brown, train_sets_full, test_sets_full)\n",
    "    print (brown_tagset_df)\n",
    "    \n",
    "    ### UNIVERSAL TAGSET\n",
    "    # Split each category in the brown corpus into train and test using tagset='universal'\n",
    "    print \"\\n ################# UNIVERSAL TAGSET ################\\n\"\n",
    "    train_sets_uni, test_sets_uni = split_each_category_in_train_and_test(brown, fraction_ts, tagset='universal')\n",
    "    universal_tagset_df, models_df_uni = train_and_evaluate_taggers(taggers, brown, train_sets_uni, test_sets_uni)\n",
    "    print (universal_tagset_df)   \n",
    "    \n",
    "    ### NB classifier\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (full tagset) \n",
    "    print \"\\n########### NAIVE BAYES CLASSIFIER ##############\\n\"\n",
    "    bigram_tagger = models_df_full.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime = time.time() - tic\n",
    "    print \"BigramTagger on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (full tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_full['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (full tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime_full = round(time.time() - tic, 2)\n",
    "    print \"Naive Bayes on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_full\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (universal tagset)\n",
    "    bigram_tagger = models_df_uni.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime = time.time() - tic\n",
    "    print \"BigramTagger on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (universal tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_uni['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (universal tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime_uni = time.time() - tic\n",
    "    print \"Naive Bayes on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_uni\n",
    "    \n",
    "    ### Cross evaluation\n",
    "    print \"\\n############## CROSS EVALUATION ####################\\n\"\n",
    "    # Cross-evaluate between categories (using universal tagset)\n",
    "    # Example: train on news_train, evaluate on the \"test\" of every other category\n",
    "    # Do this for all categories in the corpus\n",
    "    # Print the results\n",
    "    \n",
    "    print \"----DefaultTagger----\\n\"\n",
    "    print cross_evaluate_all('DefaultTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print \"\\n----AffixTagger----\\n\"\n",
    "    print cross_evaluate_all('AffixTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print \"\\n----UnigramTagger----\\n\"\n",
    "    print cross_evaluate_all('UnigramTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print \"\\n----BigramTagger----\\n\"\n",
    "    print cross_evaluate_all('BigramTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print \"\\n----TrigramTagger----\\n\"\n",
    "    print cross_evaluate_all('TrigramTagger', models_df_uni, test_sets_uni)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw2_part2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
