{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 2.1: Markov Models. Hidden Markov Models and Part of Speech Tagging.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Create a tri-gram model for generating pseudo-Trump sentences \n",
    "- load the corpus, tokenize it and obtain list of trigrams \n",
    "- define a function that obtains the counts of the \"model\" \n",
    "- define a function that generates a pseudo-sentence \n",
    "- when generating a sentence, make sure that your sentence fulfils the following requirements\n",
    "    - it is at least 5 words long\n",
    "    - the last token of the pseudo-sentence is a \".\", \"!\", or \"?\"\n",
    "    - it does not contain any other \".\", \"!\", \"?\" tokens other than the final one\n",
    "- print 5 pseudo-sentences\n",
    "\n",
    "2) Use the built-in n-gram HMM models in nltk to tag a corpus \n",
    "- load the brown corpus\n",
    "- split each category in the corpus to test and train\n",
    "- for each category in the corpus, train on the train set and evaluate on the test set the following taggers:\n",
    "    - default\n",
    "    - affix\n",
    "    - unigram\n",
    "    - bigram\n",
    "    - trigram\n",
    "    \n",
    "    Each tagger should have backoff configured on the previous tagger.\n",
    "    \n",
    "    Print the results in a table.\n",
    "    \n",
    "    \n",
    "- repeat the previous experiment using universal tagset. Print the results in a table.\n",
    "- cross evaluate between different genres (train on one category, evaluate on all the other categories). Print and compare the results\n",
    "- Only for the \"news\" portion of the corpus, compare\n",
    "    - the best berforming tagger (with backoff)\n",
    "    - the naive bayes tagger\n",
    "    \n",
    "    Compare the accuracy as well as the execution time.\n",
    "    \n",
    "    Use both the universal tagset and the full tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import codecs\n",
    "import codecs\n",
    "\n",
    "# Import taggers\n",
    "from nltk import DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk import ClassifierBasedPOSTagger\n",
    "\n",
    "# Import the brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don ’ t think I ’ m going to be real .\n",
      "I ’ m telling .\n",
      "I took a deep .\n",
      "I have to be back , you have to be using hair .\n",
      "ISIS would fill the room – thousands of people like you in this country lined up to them by their .\n"
     ]
    }
   ],
   "source": [
    "# Homework 2 part 1\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "\n",
    "import random\n",
    "\n",
    "class HMM(object):\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.db = {}\n",
    "        self.tokens = tokens\n",
    "        self.count = len(self.tokens)\n",
    "        self.createdb()\n",
    "    # train the model by couting bigram following words\n",
    "    def createdb(self):\n",
    "        trigrams = nltk.trigrams(self.tokens)\n",
    "        for w1, w2, predicted in trigrams:\n",
    "            key = (w1, w2)\n",
    "            if key in self.db:\n",
    "                self.db[key].append(predicted)\n",
    "            else:\n",
    "                self.db[key] = [predicted]\n",
    "    # generate a sentence ending with \".!?\" starting with a random bigram\n",
    "    def generate_sentence(self):\n",
    "        rng = random.randint(0, self.count-3)\n",
    "        # Start with uppercase\n",
    "        while not self.tokens[rng].isupper():\n",
    "            rng = (rng + 1) % self.count-3\n",
    "        first, second = self.tokens[rng], self.tokens[rng+1]\n",
    "        generated = []\n",
    "        for i in range(50):\n",
    "            generated.append(first)\n",
    "            first, second = second, random.choice(self.db[(first, second)])\n",
    "            if second in \".!?\":\n",
    "                break        \n",
    "        generated.append(second)\n",
    "        return generated\n",
    "    # generate x sentences of length at least 5\n",
    "    def printXSentences(self, count):\n",
    "        senCount = 0\n",
    "        while (senCount < count):\n",
    "            sentence = self.generate_sentence()\n",
    "            if len(sentence) > 4:           \n",
    "                print(' '.join(sentence))\n",
    "                senCount = senCount + 1\n",
    "\n",
    "def hw2_part1():\n",
    "    \n",
    "    # Trump speeches file location\n",
    "    fname = \"speeches.txt\"\n",
    "    # Read the corpus\n",
    "    raw_corpus = codecs.open(fname,'r','utf8').read()\n",
    "    \n",
    "    # Tokenize the corpus\n",
    "    corpus = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "    # Generate list of trigrams\n",
    "    # Initialize the \"markov model\"\n",
    "    # Preferably, you should define a function (or an object)\n",
    "    # which \"trains\" the model. You should just invoke the function here.\n",
    "    # Fill in all the counts \n",
    "\n",
    "    model = HMM(corpus)\n",
    "    \n",
    "    # Generate a sentence\n",
    "    # Preferably you should define a function (or an object)\n",
    "    # which \"generates\" a sentence following the requirements (min length, ending with punctuation, etc)\n",
    "    # You should just invoke the code here\n",
    "    \n",
    "            \n",
    "    # Print the sentences\n",
    "    model.printXSentences(5)\n",
    "    \n",
    "hw2_part1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2 part 2\n",
    "\n",
    "# Function that splits a corpus in train and test\n",
    "def split_train_test(corpus,test_size=500):\n",
    "    return corpus[test_size:], corpus[:test_size]\n",
    "\n",
    "def split_each_category_in_train_and_test(corpus, fraction_ts, tagset=None):\n",
    "    \"\"\"\n",
    "    splits each category in train and test set.\n",
    "    Returns two dictionaries: one for the training sets and one for the test sets, both indexed by the category.\n",
    "    \"\"\"\n",
    "    train_sets = {} # dictionary of training sets, one key for each category\n",
    "    test_sets = {}  # dictionary of test sets, one key for each category\n",
    "    for category in corpus.categories():\n",
    "        corpus_tsents = corpus.tagged_sents(categories=category, tagset=tagset)\n",
    "        tsents_train, tsents_test = split_train_test(corpus_tsents, test_size=int(fraction_ts * len(corpus_tsents)))\n",
    "        # add training and test set to the corresponding dictionary\n",
    "        train_sets[category] = tsents_train\n",
    "        test_sets[category] = tsents_test\n",
    "        \n",
    "    return train_sets, test_sets\n",
    "\n",
    "def get_most_common_tag(train_set):\n",
    "    \"\"\"\n",
    "    Returns the most common tag in a training set. (used by DefaultTagger)\n",
    "    \"\"\"\n",
    "    flat_list = [item for sublist in train_set for item in sublist]\n",
    "    tags = [tag for (word, tag) in flat_list]\n",
    "    # Get the most frequent tag in the training set\n",
    "    most_frequent_tag = nltk.FreqDist(tags).max()\n",
    "    return most_frequent_tag\n",
    "\n",
    "def cross_evaluate(model, test_sets):\n",
    "    \"\"\"\n",
    "    Evaluates the tagger 'model', trained on one category, on the test set of each category.\n",
    "    Subroutines in function cross_evaluate_all(). Returns test accuracies.\n",
    "    \"\"\"\n",
    "    accuracies = {'TS_' + category : round(model.evaluate(test_set),4) * 100 \n",
    "                      for (category, test_set) in test_sets.iteritems()}\n",
    "    return accuracies\n",
    "    \n",
    "def cross_evaluate_all(tagger_name, models_df_uni, test_sets_uni):\n",
    "    \"\"\"\n",
    "    Cross evaluates all tagger models with name 'tagger_name'. Each tagger is pre-trained on a single category.\n",
    "    This function tests them on all other categories. Returns a dataframe of the accuracies.\n",
    "    \"\"\"\n",
    "    model_cross_results = {'TR_' + category : [] for category in brown.categories()}\n",
    "    for category in brown.categories():\n",
    "        tagger_model = models_df_uni.loc[category, tagger_name]\n",
    "        accuracies = cross_evaluate(tagger_model, test_sets_uni)\n",
    "        model_cross_results['TR_'+category] = accuracies\n",
    "    return pd.DataFrame(model_cross_results)\n",
    "\n",
    "# For each category, train and evaluate taggers. Use backoff.\n",
    "def train_and_evaluate_taggers(taggers_list, corpus, train_sets, test_sets):\n",
    "    \"\"\"\n",
    "    Trains a list of taggers on a corpus and evaluates them. Each tagger has the previous\n",
    "    tagger as backoff. Returns the dataframes of test accuracies and the trained models.\n",
    "    \"\"\"\n",
    "    # dictionary to keep the results of each tagger on each category\n",
    "    benchmarks_df = {t.__name__: [] for t in taggers_list}\n",
    "    # dictionary to keep models trained on each category\n",
    "    models_df = {t.__name__: [] for t in taggers_list}\n",
    "    \n",
    "    for i, category in enumerate(brown.categories()):\n",
    "        for tagger in taggers_list:\n",
    "            if tagger == DefaultTagger:\n",
    "                most_common_tag = get_most_common_tag(train_sets[category])\n",
    "                # the default tagger does not have a backoff tagger\n",
    "                tagger_model = tagger(most_common_tag)\n",
    "            else:\n",
    "                # each tagger takes as backoff tagger the previous tagger\n",
    "                tagger_model = tagger(train_sets[category], backoff=tagger_model)\n",
    "            # evaluate tagger\n",
    "            accuracy = round(tagger_model.evaluate(test_sets[category]),4) * 100\n",
    "            # append to dictionaries accuracy and model\n",
    "            benchmarks_df[tagger.__name__].append(accuracy)\n",
    "            models_df[tagger.__name__].append(tagger_model)\n",
    "    \n",
    "    # returns the statistics for all taggers and all categories\n",
    "    benchmarks_df = pd.DataFrame(benchmarks_df, index=brown.categories())\n",
    "    models_df = pd.DataFrame(models_df, index=brown.categories())\n",
    "    return benchmarks_df, models_df\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "def hw2_part2():\n",
    "    # list of taggers to benchmark\n",
    "    taggers = [DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger]\n",
    "    fraction_ts = 0.2 # faction test set\n",
    "    \n",
    "    ### BROWN TAGSET   \n",
    "    # Split each category in the brown corpus into train and test\n",
    "    print (\"############## BROWN TAGSET ##################\\n\")\n",
    "    train_sets_full, test_sets_full = split_each_category_in_train_and_test(brown, fraction_ts)\n",
    "    brown_tagset_df, models_df_full = train_and_evaluate_taggers(taggers, brown, train_sets_full, test_sets_full)\n",
    "    print (brown_tagset_df)\n",
    "    \n",
    "    ### UNIVERSAL TAGSET\n",
    "    # Split each category in the brown corpus into train and test using tagset='universal'\n",
    "    print (\"\\n ################# UNIVERSAL TAGSET ################\\n\")\n",
    "    train_sets_uni, test_sets_uni = split_each_category_in_train_and_test(brown, fraction_ts, tagset='universal')\n",
    "    universal_tagset_df, models_df_uni = train_and_evaluate_taggers(taggers, brown, train_sets_uni, test_sets_uni)\n",
    "    print (universal_tagset_df)   \n",
    "    \n",
    "    ### NB classifier\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (full tagset) \n",
    "    print (\"\\n########### NAIVE BAYES CLASSIFIER ##############\\n\")\n",
    "    bigram_tagger = models_df_full.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime = time.time() - tic\n",
    "    print (\"BigramTagger on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime)\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (full tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_full['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (full tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime_full = round(time.time() - tic, 2)\n",
    "    print (\"Naive Bayes on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_full)\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (universal tagset)\n",
    "    bigram_tagger = models_df_uni.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime = time.time() - tic\n",
    "    print (\"BigramTagger on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime)\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (universal tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_uni['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (universal tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime_uni = time.time() - tic\n",
    "    print (\"Naive Bayes on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_uni)\n",
    "    \n",
    "    ### Cross evaluation\n",
    "    print (\"\\n############## CROSS EVALUATION ####################\\n\")\n",
    "    # Cross-evaluate between categories (using universal tagset)\n",
    "    # Example: train on news_train, evaluate on the \"test\" of every other category\n",
    "    # Do this for all categories in the corpus\n",
    "    # Print the results\n",
    "    \n",
    "    print (\"----DefaultTagger----\\n\")\n",
    "    print cross_evaluate_all('DefaultTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print (\"\\n----AffixTagger----\\n\")\n",
    "    print cross_evaluate_all('AffixTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print (\"\\n----UnigramTagger----\\n\")\n",
    "    print cross_evaluate_all('UnigramTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print (\"\\n----BigramTagger----\\n\")\n",
    "    print cross_evaluate_all('BigramTagger', models_df_uni, test_sets_uni)\n",
    "    \n",
    "    print (\"\\n----TrigramTagger----\\n\")\n",
    "    print cross_evaluate_all('TrigramTagger', models_df_uni, test_sets_uni)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############## BROWN TAGSET ##################\n",
      "\n",
      "                 AffixTagger  BigramTagger  DefaultTagger  TrigramTagger  \\\n",
      "adventure              23.71         90.17          10.87          90.05   \n",
      "belles_lettres         28.15         90.38          12.86          90.34   \n",
      "editorial              29.28         87.09          13.93          86.85   \n",
      "fiction                24.46         88.74          11.38          88.69   \n",
      "government             31.51         86.49          13.67          86.48   \n",
      "hobbies                28.19         85.97          13.63          85.94   \n",
      "humor                  24.44         83.24          11.73          83.33   \n",
      "learned                34.94         90.24          18.04          90.31   \n",
      "lore                   27.94         88.32          14.05          88.18   \n",
      "mystery                23.33         89.47          11.30          89.25   \n",
      "news                   29.60         88.31          14.05          88.31   \n",
      "religion               27.70         87.89          13.13          87.99   \n",
      "reviews                27.00         85.15          12.20          85.22   \n",
      "romance                23.73         87.17          10.37          87.16   \n",
      "science_fiction        19.11         80.51           8.01          80.74   \n",
      "\n",
      "                 UnigramTagger  \n",
      "adventure                88.79  \n",
      "belles_lettres           89.11  \n",
      "editorial                86.12  \n",
      "fiction                  87.44  \n",
      "government               85.25  \n",
      "hobbies                  85.01  \n",
      "humor                    82.82  \n",
      "learned                  89.19  \n",
      "lore                     87.16  \n",
      "mystery                  87.53  \n",
      "news                     87.54  \n",
      "religion                 87.08  \n",
      "reviews                  84.57  \n",
      "romance                  86.22  \n",
      "science_fiction          79.77  \n",
      "\n",
      " ################# UNIVERSAL TAGSET ################\n",
      "\n",
      "                 AffixTagger  BigramTagger  DefaultTagger  TrigramTagger  \\\n",
      "adventure              28.72         94.14          17.91          93.96   \n",
      "belles_lettres         34.24         94.09          22.31          93.96   \n",
      "editorial              37.85         92.96          27.09          92.69   \n",
      "fiction                30.93         93.37          19.79          93.23   \n",
      "government             40.23         93.49          27.99          93.61   \n",
      "hobbies                34.47         91.35          23.97          91.41   \n",
      "humor                  32.77         90.97          22.04          90.68   \n",
      "learned                39.74         94.46          27.05          94.52   \n",
      "lore                   34.48         92.75          23.64          92.67   \n",
      "mystery                29.16         93.56          17.97          93.50   \n",
      "news                   41.00         94.52          30.62          94.25   \n",
      "religion               33.64         92.22          21.15          92.22   \n",
      "reviews                36.37         92.46          25.80          92.15   \n",
      "romance                32.84         89.94          18.49          89.70   \n",
      "science_fiction        27.72         88.97          17.40          89.01   \n",
      "\n",
      "                 UnigramTagger  \n",
      "adventure                93.27  \n",
      "belles_lettres           93.59  \n",
      "editorial                92.39  \n",
      "fiction                  92.91  \n",
      "government               93.03  \n",
      "hobbies                  90.93  \n",
      "humor                    90.84  \n",
      "learned                  93.73  \n",
      "lore                     92.27  \n",
      "mystery                  92.66  \n",
      "news                     94.13  \n",
      "religion                 92.02  \n",
      "reviews                  92.30  \n",
      "romance                  89.62  \n",
      "science_fiction          89.12  \n",
      "\n",
      "########### NAIVE BAYES CLASSIFIER ##############\n",
      "\n",
      "('BigramTagger on news, (full tagset). Accuracy:', 88.31, 'Evaluation runtime (s):', 0.1496870517730713)\n",
      "('Naive Bayes on news, (full tagset). Accuracy:', 89.71, 'Evaluation runtime (s):', 77.45)\n",
      "('BigramTagger on news, (universal tagset). Accuracy:', 94.52, 'Evaluation runtime (s):', 0.19105887413024902)\n",
      "('Naive Bayes on news, (universal tagset). Accuracy:', 91.94, 'Evaluation runtime (s):', 4.771721124649048)\n",
      "\n",
      "############## CROSS EVALUATION ####################\n",
      "\n",
      "----DefaultTagger----\n",
      "\n",
      "                    TR_adventure  TR_belles_lettres  TR_editorial  TR_fiction  \\\n",
      "TS_adventure               17.91              17.91         17.91       17.91   \n",
      "TS_belles_lettres          22.31              22.31         22.31       22.31   \n",
      "TS_editorial               27.09              27.09         27.09       27.09   \n",
      "TS_fiction                 19.79              19.79         19.79       19.79   \n",
      "TS_government              27.99              27.99         27.99       27.99   \n",
      "TS_hobbies                 23.97              23.97         23.97       23.97   \n",
      "TS_humor                   22.04              22.04         22.04       22.04   \n",
      "TS_learned                 27.05              27.05         27.05       27.05   \n",
      "TS_lore                    23.64              23.64         23.64       23.64   \n",
      "TS_mystery                 17.97              17.97         17.97       17.97   \n",
      "TS_news                    30.62              30.62         30.62       30.62   \n",
      "TS_religion                21.15              21.15         21.15       21.15   \n",
      "TS_reviews                 25.80              25.80         25.80       25.80   \n",
      "TS_romance                 19.04              19.04         19.04       19.04   \n",
      "TS_science_fiction         17.40              17.40         17.40       17.40   \n",
      "\n",
      "                    TR_government  TR_hobbies  TR_humor  TR_learned  TR_lore  \\\n",
      "TS_adventure                17.91       17.91     17.91       17.91    17.91   \n",
      "TS_belles_lettres           22.31       22.31     22.31       22.31    22.31   \n",
      "TS_editorial                27.09       27.09     27.09       27.09    27.09   \n",
      "TS_fiction                  19.79       19.79     19.79       19.79    19.79   \n",
      "TS_government               27.99       27.99     27.99       27.99    27.99   \n",
      "TS_hobbies                  23.97       23.97     23.97       23.97    23.97   \n",
      "TS_humor                    22.04       22.04     22.04       22.04    22.04   \n",
      "TS_learned                  27.05       27.05     27.05       27.05    27.05   \n",
      "TS_lore                     23.64       23.64     23.64       23.64    23.64   \n",
      "TS_mystery                  17.97       17.97     17.97       17.97    17.97   \n",
      "TS_news                     30.62       30.62     30.62       30.62    30.62   \n",
      "TS_religion                 21.15       21.15     21.15       21.15    21.15   \n",
      "TS_reviews                  25.80       25.80     25.80       25.80    25.80   \n",
      "TS_romance                  19.04       19.04     19.04       19.04    19.04   \n",
      "TS_science_fiction          17.40       17.40     17.40       17.40    17.40   \n",
      "\n",
      "                    TR_mystery  TR_news  TR_religion  TR_reviews  TR_romance  \\\n",
      "TS_adventure             17.91    17.91        17.91       17.91       18.85   \n",
      "TS_belles_lettres        22.31    22.31        22.31       22.31       15.04   \n",
      "TS_editorial             27.09    27.09        27.09       27.09       16.15   \n",
      "TS_fiction               19.79    19.79        19.79       19.79       17.85   \n",
      "TS_government            27.99    27.99        27.99       27.99       14.00   \n",
      "TS_hobbies               23.97    23.97        23.97       23.97       16.72   \n",
      "TS_humor                 22.04    22.04        22.04       22.04       16.91   \n",
      "TS_learned               27.05    27.05        27.05       27.05       14.28   \n",
      "TS_lore                  23.64    23.64        23.64       23.64       15.94   \n",
      "TS_mystery               17.97    17.97        17.97       17.97       18.38   \n",
      "TS_news                  30.62    30.62        30.62       30.62       16.00   \n",
      "TS_religion              21.15    21.15        21.15       21.15       15.36   \n",
      "TS_reviews               25.80    25.80        25.80       25.80       13.58   \n",
      "TS_romance               19.04    19.04        19.04       19.04       18.49   \n",
      "TS_science_fiction       17.40    17.40        17.40       17.40       19.00   \n",
      "\n",
      "                    TR_science_fiction  \n",
      "TS_adventure                     17.91  \n",
      "TS_belles_lettres                22.31  \n",
      "TS_editorial                     27.09  \n",
      "TS_fiction                       19.79  \n",
      "TS_government                    27.99  \n",
      "TS_hobbies                       23.97  \n",
      "TS_humor                         22.04  \n",
      "TS_learned                       27.05  \n",
      "TS_lore                          23.64  \n",
      "TS_mystery                       17.97  \n",
      "TS_news                          30.62  \n",
      "TS_religion                      21.15  \n",
      "TS_reviews                       25.80  \n",
      "TS_romance                       19.04  \n",
      "TS_science_fiction               17.40  \n",
      "\n",
      "----AffixTagger----\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TR_adventure  TR_belles_lettres  TR_editorial  TR_fiction  \\\n",
      "TS_adventure               28.72              27.88         27.94       28.72   \n",
      "TS_belles_lettres          33.14              34.24         33.99       33.06   \n",
      "TS_editorial               36.47              37.80         37.85       36.62   \n",
      "TS_fiction                 30.79              30.31         30.36       30.93   \n",
      "TS_government              38.07              39.60         39.52       38.11   \n",
      "TS_hobbies                 33.67              34.38         34.36       33.54   \n",
      "TS_humor                   32.93              33.49         32.86       33.31   \n",
      "TS_learned                 37.39              39.43         39.03       37.71   \n",
      "TS_lore                    33.83              34.30         34.13       33.51   \n",
      "TS_mystery                 28.75              28.03         28.54       28.88   \n",
      "TS_news                    38.74              40.10         40.42       39.20   \n",
      "TS_religion                32.11              33.88         33.64       32.70   \n",
      "TS_reviews                 35.21              36.14         35.73       35.21   \n",
      "TS_romance                 30.21              30.30         30.35       30.48   \n",
      "TS_science_fiction         27.65              28.13         28.09       28.17   \n",
      "\n",
      "                    TR_government  TR_hobbies  TR_humor  TR_learned  TR_lore  \\\n",
      "TS_adventure                26.79       28.15     27.79       27.32    28.16   \n",
      "TS_belles_lettres           32.89       33.40     32.87       34.32    34.21   \n",
      "TS_editorial                37.31       37.28     36.34       37.84    38.31   \n",
      "TS_fiction                  29.19       30.10     30.00       30.20    30.33   \n",
      "TS_government               40.23       39.34     37.60       40.50    39.78   \n",
      "TS_hobbies                  33.71       34.47     33.25       34.37    34.34   \n",
      "TS_humor                    31.67       32.70     32.77       32.86    33.42   \n",
      "TS_learned                  38.74       39.09     37.72       39.74    39.40   \n",
      "TS_lore                     33.37       33.56     33.18       34.30    34.48   \n",
      "TS_mystery                  27.07       28.45     27.89       27.97    28.58   \n",
      "TS_news                     40.08       39.53     38.16       40.24    40.59   \n",
      "TS_religion                 32.52       32.04     32.55       33.63    33.70   \n",
      "TS_reviews                  34.92       35.61     35.09       35.76    36.28   \n",
      "TS_romance                  29.20       29.59     29.67       29.99    30.31   \n",
      "TS_science_fiction          27.31       27.57     27.94       27.79    28.46   \n",
      "\n",
      "                    TR_mystery  TR_news  TR_religion  TR_reviews  TR_romance  \\\n",
      "TS_adventure             28.90    28.04        27.27       27.27       30.91   \n",
      "TS_belles_lettres        33.30    33.64        33.88       33.81       34.94   \n",
      "TS_editorial             36.32    38.13        37.03       36.95       37.25   \n",
      "TS_fiction               30.65    30.03        30.03       29.94       33.45   \n",
      "TS_government            38.14    39.52        38.80       38.43       39.13   \n",
      "TS_hobbies               33.57    34.18        34.11       34.13       34.67   \n",
      "TS_humor                 33.13    32.84        32.41       32.52       33.81   \n",
      "TS_learned               37.86    38.79        38.26       38.84       36.17   \n",
      "TS_lore                  33.31    33.75        33.52       33.94       34.46   \n",
      "TS_mystery               29.16    28.74        27.88       28.06       31.66   \n",
      "TS_news                  39.15    41.00        39.73       39.08       37.98   \n",
      "TS_religion              32.26    32.39        33.64       33.17       35.34   \n",
      "TS_reviews               34.55    35.90        35.38       36.37       33.70   \n",
      "TS_romance               30.38    30.15        29.91       29.80       32.84   \n",
      "TS_science_fiction       28.58    27.35        28.20       27.79       31.59   \n",
      "\n",
      "                    TR_science_fiction  \n",
      "TS_adventure                     27.95  \n",
      "TS_belles_lettres                33.09  \n",
      "TS_editorial                     36.17  \n",
      "TS_fiction                       29.75  \n",
      "TS_government                    38.14  \n",
      "TS_hobbies                       33.28  \n",
      "TS_humor                         32.32  \n",
      "TS_learned                       38.21  \n",
      "TS_lore                          32.97  \n",
      "TS_mystery                       28.05  \n",
      "TS_news                          38.68  \n",
      "TS_religion                      32.74  \n",
      "TS_reviews                       34.46  \n",
      "TS_romance                       29.75  \n",
      "TS_science_fiction               27.72  \n",
      "\n",
      "----UnigramTagger----\n",
      "\n",
      "                    TR_adventure  TR_belles_lettres  TR_editorial  TR_fiction  \\\n",
      "TS_adventure               93.27              92.80         91.38       93.23   \n",
      "TS_belles_lettres          90.60              93.59         92.08       91.14   \n",
      "TS_editorial               89.61              93.10         92.39       89.86   \n",
      "TS_fiction                 92.88              93.39         92.08       92.91   \n",
      "TS_government              89.89              93.43         92.50       89.40   \n",
      "TS_hobbies                 88.79              91.35         90.07       88.63   \n",
      "TS_humor                   92.72              94.03         92.97       93.28   \n",
      "TS_learned                 89.05              92.69         91.79       88.68   \n",
      "TS_lore                    90.78              92.74         91.50       90.31   \n",
      "TS_mystery                 92.75              92.41         91.41       92.53   \n",
      "TS_news                    90.67              93.72         93.13       90.39   \n",
      "TS_religion                90.53              94.20         92.55       91.42   \n",
      "TS_reviews                 90.30              93.24         91.62       90.72   \n",
      "TS_romance                 91.93              92.35         91.25       92.09   \n",
      "TS_science_fiction         91.13              92.25         91.17       91.54   \n",
      "\n",
      "                    TR_government  TR_hobbies  TR_humor  TR_learned  TR_lore  \\\n",
      "TS_adventure                87.49       91.65     90.20       91.25    92.52   \n",
      "TS_belles_lettres           89.96       91.61     89.09       93.00    92.97   \n",
      "TS_editorial                90.68       91.47     88.48       92.92    92.64   \n",
      "TS_fiction                  88.79       91.57     90.22       92.09    92.83   \n",
      "TS_government               93.03       92.95     88.53       93.72    93.08   \n",
      "TS_hobbies                  88.90       90.93     87.14       91.22    91.26   \n",
      "TS_humor                    89.40       92.36     90.84       92.27    93.35   \n",
      "TS_learned                  91.89       92.46     87.49       93.73    92.63   \n",
      "TS_lore                     88.84       91.19     88.78       92.17    92.27   \n",
      "TS_mystery                  86.73       91.04     90.27       91.01    92.34   \n",
      "TS_news                     91.79       92.47     88.65       93.43    93.44   \n",
      "TS_religion                 91.90       91.96     89.90       93.53    93.59   \n",
      "TS_reviews                  89.25       91.30     89.11       92.30    92.74   \n",
      "TS_romance                  86.52       90.91     89.73       91.08    92.07   \n",
      "TS_science_fiction          86.66       90.83     89.61       90.35    91.39   \n",
      "\n",
      "                    TR_mystery  TR_news  TR_religion  TR_reviews  TR_romance  \\\n",
      "TS_adventure             92.69    92.53        89.02       90.06       91.12   \n",
      "TS_belles_lettres        90.31    91.82        90.68       91.34       88.46   \n",
      "TS_editorial             89.15    92.73        90.07       90.88       86.27   \n",
      "TS_fiction               92.51    91.92        90.42       90.86       90.71   \n",
      "TS_government            89.28    92.48        90.61       91.45       87.06   \n",
      "TS_hobbies               88.03    90.64        88.65       89.27       85.70   \n",
      "TS_humor                 92.61    93.37        90.93       91.69       89.69   \n",
      "TS_learned               88.53    92.19        89.90       91.02       83.09   \n",
      "TS_lore                  89.90    91.80        89.82       90.60       87.80   \n",
      "TS_mystery               92.66    91.89        89.08       90.33       91.21   \n",
      "TS_news                  90.48    94.13        91.12       90.74       85.79   \n",
      "TS_religion              90.09    92.27        92.02       91.88       89.37   \n",
      "TS_reviews               89.28    92.03        89.77       92.30       85.47   \n",
      "TS_romance               91.79    91.71        89.19       90.30       89.62   \n",
      "TS_science_fiction       91.58    90.76        89.16       90.09       89.12   \n",
      "\n",
      "                    TR_science_fiction  \n",
      "TS_adventure                     89.74  \n",
      "TS_belles_lettres                88.37  \n",
      "TS_editorial                     86.90  \n",
      "TS_fiction                       89.37  \n",
      "TS_government                    87.30  \n",
      "TS_hobbies                       86.38  \n",
      "TS_humor                         89.78  \n",
      "TS_learned                       87.23  \n",
      "TS_lore                          87.78  \n",
      "TS_mystery                       89.51  \n",
      "TS_news                          87.74  \n",
      "TS_religion                      89.23  \n",
      "TS_reviews                       87.43  \n",
      "TS_romance                       88.82  \n",
      "TS_science_fiction               89.12  \n",
      "\n",
      "----BigramTagger----\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TR_adventure  TR_belles_lettres  TR_editorial  TR_fiction  \\\n",
      "TS_adventure               94.14              93.45         91.64       93.74   \n",
      "TS_belles_lettres          90.91              94.09         92.53       91.39   \n",
      "TS_editorial               89.80              93.62         92.96       90.24   \n",
      "TS_fiction                 93.41              93.93         92.36       93.37   \n",
      "TS_government              89.93              93.81         92.94       89.43   \n",
      "TS_hobbies                 89.14              91.95         90.61       88.93   \n",
      "TS_humor                   93.10              94.43         93.10       93.55   \n",
      "TS_learned                 89.27              93.04         92.11       88.85   \n",
      "TS_lore                    91.06              93.31         91.95       90.51   \n",
      "TS_mystery                 93.67              93.12         91.76       93.08   \n",
      "TS_news                    90.86              94.21         93.69       90.51   \n",
      "TS_religion                90.80              94.81         92.93       91.35   \n",
      "TS_reviews                 90.55              93.46         91.82       91.12   \n",
      "TS_romance                 92.51              92.90         91.54       92.42   \n",
      "TS_science_fiction         91.02              92.47         91.24       91.39   \n",
      "\n",
      "                    TR_government  TR_hobbies  TR_humor  TR_learned  TR_lore  \\\n",
      "TS_adventure                88.18       92.08     90.60       91.59    93.14   \n",
      "TS_belles_lettres           90.70       91.92     89.17       93.52    93.50   \n",
      "TS_editorial                91.31       91.71     88.41       93.16    93.16   \n",
      "TS_fiction                  89.72       91.93     90.38       92.40    93.31   \n",
      "TS_government               93.49       93.36     88.34       94.39    93.52   \n",
      "TS_hobbies                  89.60       91.35     87.20       91.83    91.88   \n",
      "TS_humor                    89.98       92.61     90.97       92.57    93.33   \n",
      "TS_learned                  92.32       93.00     87.47       94.46    93.09   \n",
      "TS_lore                     89.38       91.44     88.78       92.60    92.75   \n",
      "TS_mystery                  87.48       91.88     90.78       91.50    93.01   \n",
      "TS_news                     92.34       92.33     88.51       93.68    93.82   \n",
      "TS_religion                 92.30       92.36     89.69       94.09    93.97   \n",
      "TS_reviews                  89.83       91.73     89.28       92.45    93.44   \n",
      "TS_romance                  87.26       91.19     89.98       91.18    92.64   \n",
      "TS_science_fiction          87.52       90.91     89.31       90.24    91.51   \n",
      "\n",
      "                    TR_mystery  TR_news  TR_religion  TR_reviews  TR_romance  \\\n",
      "TS_adventure             93.52    92.92        89.51       90.37       91.70   \n",
      "TS_belles_lettres        90.76    92.12        90.89       91.67       88.83   \n",
      "TS_editorial             89.54    93.16        90.18       90.84       86.50   \n",
      "TS_fiction               93.13    92.08        90.76       90.96       91.15   \n",
      "TS_government            89.74    93.00        90.65       91.60       87.21   \n",
      "TS_hobbies               88.62    91.16        88.84       89.59       85.98   \n",
      "TS_humor                 93.06    93.69        90.97       91.91       89.91   \n",
      "TS_learned               89.00    92.59        90.11       91.30       83.42   \n",
      "TS_lore                  90.65    92.26        89.98       90.75       88.04   \n",
      "TS_mystery               93.56    92.58        89.43       90.54       91.82   \n",
      "TS_news                  90.86    94.52        91.21       90.68       85.98   \n",
      "TS_religion              90.59    92.72        92.22       91.93       89.37   \n",
      "TS_reviews               89.84    92.19        89.99       92.46       85.88   \n",
      "TS_romance               92.34    92.11        89.46       90.10       89.94   \n",
      "TS_science_fiction       91.47    91.06        89.38       89.87       89.05   \n",
      "\n",
      "                    TR_science_fiction  \n",
      "TS_adventure                     89.68  \n",
      "TS_belles_lettres                88.48  \n",
      "TS_editorial                     86.72  \n",
      "TS_fiction                       89.53  \n",
      "TS_government                    87.47  \n",
      "TS_hobbies                       86.49  \n",
      "TS_humor                         89.91  \n",
      "TS_learned                       87.54  \n",
      "TS_lore                          87.69  \n",
      "TS_mystery                       89.78  \n",
      "TS_news                          87.47  \n",
      "TS_religion                      89.28  \n",
      "TS_reviews                       87.38  \n",
      "TS_romance                       88.81  \n",
      "TS_science_fiction               88.97  \n",
      "\n",
      "----TrigramTagger----\n",
      "\n",
      "                    TR_adventure  TR_belles_lettres  TR_editorial  TR_fiction  \\\n",
      "TS_adventure               93.96              93.36         91.62       93.72   \n",
      "TS_belles_lettres          90.79              93.96         92.46       91.26   \n",
      "TS_editorial               89.62              93.37         92.69       90.13   \n",
      "TS_fiction                 93.34              93.80         92.13       93.23   \n",
      "TS_government              89.92              93.86         92.74       89.43   \n",
      "TS_hobbies                 89.03              91.93         90.55       88.75   \n",
      "TS_humor                   93.10              94.20         92.92       93.33   \n",
      "TS_learned                 89.20              93.15         92.12       88.74   \n",
      "TS_lore                    90.97              93.26         91.87       90.47   \n",
      "TS_mystery                 93.69              93.12         91.75       93.19   \n",
      "TS_news                    90.65              93.97         93.48       90.36   \n",
      "TS_religion                90.65              94.65         92.70       91.12   \n",
      "TS_reviews                 90.27              93.45         91.77       90.88   \n",
      "TS_romance                 92.43              92.72         91.24       92.38   \n",
      "TS_science_fiction         91.06              92.03         91.02       91.21   \n",
      "\n",
      "                    TR_government  TR_hobbies  TR_humor  TR_learned  TR_lore  \\\n",
      "TS_adventure                88.34       92.21     90.44       91.56    93.01   \n",
      "TS_belles_lettres           90.73       91.77     89.06       93.50    93.43   \n",
      "TS_editorial                91.38       91.79     88.31       93.20    93.04   \n",
      "TS_fiction                  89.76       91.92     90.18       92.52    93.13   \n",
      "TS_government               93.61       93.32     88.19       94.24    93.45   \n",
      "TS_hobbies                  89.73       91.41     87.07       91.86    91.81   \n",
      "TS_humor                    89.89       92.12     90.68       92.34    93.19   \n",
      "TS_learned                  92.39       92.89     87.43       94.52    93.19   \n",
      "TS_lore                     89.64       91.37     88.71       92.63    92.67   \n",
      "TS_mystery                  87.57       91.95     90.61       91.60    92.95   \n",
      "TS_news                     92.67       92.41     88.34       93.93    93.82   \n",
      "TS_religion                 92.35       92.20     89.46       93.91    93.73   \n",
      "TS_reviews                  89.97       91.62     89.08       92.53    93.20   \n",
      "TS_romance                  87.38       91.31     89.93       91.28    92.48   \n",
      "TS_science_fiction          87.74       90.83     89.16       90.13    91.17   \n",
      "\n",
      "                    TR_mystery  TR_news  TR_religion  TR_reviews  TR_romance  \\\n",
      "TS_adventure             93.45    92.65        89.34       90.33       91.52   \n",
      "TS_belles_lettres        90.56    92.07        90.78       91.47       88.53   \n",
      "TS_editorial             89.20    93.01        89.94       90.63       86.18   \n",
      "TS_fiction               92.97    91.92        90.62       90.74       90.84   \n",
      "TS_government            89.53    92.84        90.73       91.45       87.09   \n",
      "TS_hobbies               88.55    91.22        88.89       89.35       85.66   \n",
      "TS_humor                 92.65    93.26        90.99       91.71       89.78   \n",
      "TS_learned               88.82    92.60        90.31       91.06       83.22   \n",
      "TS_lore                  90.36    92.26        89.83       90.60       87.73   \n",
      "TS_mystery               93.50    92.43        89.41       90.33       91.60   \n",
      "TS_news                  90.35    94.25        91.14       90.52       85.64   \n",
      "TS_religion              90.23    92.49        92.22       91.67       89.16   \n",
      "TS_reviews               89.48    92.41        89.83       92.15       85.47   \n",
      "TS_romance               92.10    91.98        89.38       90.07       89.70   \n",
      "TS_science_fiction       91.32    90.54        89.01       89.79       88.93   \n",
      "\n",
      "                    TR_science_fiction  \n",
      "TS_adventure                     89.69  \n",
      "TS_belles_lettres                88.32  \n",
      "TS_editorial                     86.68  \n",
      "TS_fiction                       89.32  \n",
      "TS_government                    87.28  \n",
      "TS_hobbies                       86.47  \n",
      "TS_humor                         89.49  \n",
      "TS_learned                       87.40  \n",
      "TS_lore                          87.55  \n",
      "TS_mystery                       89.60  \n",
      "TS_news                          87.41  \n",
      "TS_religion                      89.23  \n",
      "TS_reviews                       87.41  \n",
      "TS_romance                       88.88  \n",
      "TS_science_fiction               89.01  \n"
     ]
    }
   ],
   "source": [
    "hw2_part2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
