{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import chain\n",
    "from time import time\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_plus_valid_perc = .2\n",
    "valid_of_test_perc = .5\n",
    "\n",
    "data = []\n",
    "# create a useful dictionary from each review\n",
    "for category in mr.categories():\n",
    "\n",
    "    if category == 'pos':\n",
    "        pretty_category_name = 'positive'\n",
    "    elif category == 'neg':\n",
    "        pretty_category_name = 'negative'\n",
    "\n",
    "    for fileid in mr.fileids(category):\n",
    "\n",
    "        review_words = mr.words(fileid)\n",
    "        review_text = ''\n",
    "        for word in review_words:\n",
    "            review_text += ' ' + word\n",
    "\n",
    "        review_dictionary = {\n",
    "            'text': review_text,\n",
    "            'sentiment': pretty_category_name\n",
    "        }\n",
    "\n",
    "        data.append(review_dictionary)\n",
    "        \n",
    "# create a useful dictionary from each review\n",
    "train, test = train_test_split(data, test_size=test_plus_valid_perc)\n",
    "test, valid = train_test_split(test, test_size=valid_of_test_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(dictionaries):\n",
    "    x = []\n",
    "    y = []\n",
    "    for review in dictionaries:\n",
    "        x.append(review['text'])\n",
    "        y.append(1 if review['sentiment'] == 'positive' else 0)\n",
    "    return x, y\n",
    "\n",
    " # create simple train and test and validation x - y arrays   \n",
    "train_x, train_y = get_arrays(train) \n",
    "test_x, test_y = get_arrays(test) \n",
    "valid_x, valid_y = get_arrays(valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_analyze(pipeline, x_train, y_train, x_test, y_test, x_valid, y_valid):\n",
    "    # tic\n",
    "    t0 = time()\n",
    "    \n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred_test = sentiment_fit.predict(x_test)\n",
    "    y_pred_train = sentiment_fit.predict(x_train)\n",
    "    y_pred_valid = sentiment_fit.predict(x_valid)\n",
    "    \n",
    "    # toc\n",
    "    train_test_time = time() - t0\n",
    "    \n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_valid = accuracy_score(y_valid, y_pred_valid)\n",
    "    print (\"accuracy score test: {0:.2f}%\".format(accuracy_test*100))\n",
    "    print (\"accuracy score train: {0:.2f}%\".format(accuracy_train*100))\n",
    "    print (\"accuracy score valid: {0:.2f}%\".format(accuracy_valid*100))\n",
    "    print (\"time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*50)\n",
    "    \n",
    "    return sentiment_fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_network(optimizer='rmsprop', init='glorot_uniform', dropout=0.5):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "svm = SVC()\n",
    "neural_network = KerasClassifier(create_network, epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "models = [logistic_regression, svm, neural_network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(min_df=2)\n",
    "svd = TruncatedSVD(n_components=100) # if you change n_component you need to change input_dim of the network\n",
    "\n",
    "ngram_size = 2\n",
    "features = range(200, 400, 100) #range(500, 5000, 500)\n",
    "\n",
    "for model in models:\n",
    "    print (model.__class__.__name__)\n",
    "    for numFeatures in features:\n",
    "        result = []\n",
    "        tvec.set_params(stop_words=stopwords.words('english'), max_features=numFeatures, ngram_range=(1, ngram_size))\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', tvec),\n",
    "            ('svd', svd),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "    \n",
    "        print (\"Result for {} features\".format(numFeatures))\n",
    "        fit_and_analyze(pipeline, train_x, train_y, test_x, test_y, valid_x, valid_y)\n",
    "        \n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
