{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 3: Distributional semantic models.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Obtain co-occurrence vector representations with the followin properties:\n",
    "- window size 1, pmi, svd (50)\n",
    "- window size 3, no modifications\n",
    "- window size 3, pmi, no svd\n",
    "- window size 3, no pmi, svd (50)\n",
    "- window size 3, pmi, svd (50)\n",
    "\n",
    "2) Obtain word2vec embeddings with the following properties\n",
    "- window size 1, 50 dimensions\n",
    "- window size 1, 200 dimensions\n",
    "- window size 3, 50 dimensions\n",
    "- window size 3, 200 dimensions\n",
    "- window size 5, 50 dimensions\n",
    "\n",
    "3) Compare the performance of the 10 representations in 1 and 2 on the following tasks:\n",
    "- similarity between \"man\" and \"woman\"\n",
    "- the 5 most similar words to \"car\"\n",
    "- for DISSECT representations , correlation with gold standard\n",
    "- for Word2Vec, the similarity between \"queen\" and \"king + woman - man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import spatial\n",
    "\n",
    "# Dissect\n",
    "from composes.semantic_space.space import Space\n",
    "from composes.utils import io_utils\n",
    "from composes.transformation.scaling.ppmi_weighting import PpmiWeighting\n",
    "from composes.transformation.dim_reduction.svd import Svd\n",
    "from composes.similarity.cos import CosSimilarity\n",
    "from composes.utils import scoring_utils\n",
    "\n",
    "# Gensim\n",
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the corpus\n",
    "corpus = gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSpace(windowSize):\n",
    "    ## generate the raw co-occurrence count within a window of 1\n",
    "    cooc = BigramCollocationFinder.from_words(corpus,window_size=windowSize + 1).ngram_fd.items()\n",
    "\n",
    "    ## convert the list of collocates in a dictionary\n",
    "\n",
    "    # Initialize the dict\n",
    "    cooc_dict = {}\n",
    "    print(\"Going to loop through the list\")\n",
    "    # Loop through the list\n",
    "    for pair,freq in cooc:\n",
    "        # Check and initialie the variables\n",
    "        word1,word2 = pair\n",
    "        # Check if entries for the words exist\n",
    "        # If not, create them\n",
    "        if word1 not in cooc_dict:\n",
    "            cooc_dict[word1]={}\n",
    "\n",
    "        if word2 not in cooc_dict:\n",
    "            cooc_dict[word2]={}\n",
    "\n",
    "        # Check if entries for the particular combination exists\n",
    "        # If not, initialize them\n",
    "        if word2 not in cooc_dict[word1]:\n",
    "            cooc_dict[word1][word2]=0\n",
    "        if word1 not in cooc_dict[word2]:\n",
    "            cooc_dict[word2][word1]=0\n",
    "        # Update the dict variables\n",
    "        cooc_dict[word1][word2]+=freq\n",
    "        cooc_dict[word2][word1]+=freq\n",
    "\n",
    "    ## Generate the row, col and data variables for the DISSECT\n",
    "\n",
    "    # Initialize the variables\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    print(\"Going to loop through the dictionary\")\n",
    "    # Loop through the dictionary\n",
    "    for word_1 in cooc_dict:\n",
    "        # Add an entry to the rows variable\n",
    "        # there should be no duplications, but we check anyway\n",
    "        if word_1 not in rows:\n",
    "            rows.append(word_1)\n",
    "        # Loop through the entries in the dict\n",
    "        for word_2 in cooc_dict[word_1]:\n",
    "            # Add an entry in the cols, if it's not already added\n",
    "            if word_2 not in cols:\n",
    "                cols.append(word_2)\n",
    "            # Add the value to the data\n",
    "            data.append(word_1 + \" \" + word_2 + \" \" + str(cooc_dict[word_1][word_2]))\n",
    "\n",
    "    ## Output the row,col,data to files\n",
    "\n",
    "    # Define the base name\n",
    "    fname = \"gutenberg_surface_3\"\n",
    "\n",
    "    # Generate tuples of fname data for the files\n",
    "    out = []\n",
    "    out.append((fname + \".rows\",rows))\n",
    "    out.append((fname + \".cols\",cols))\n",
    "    out.append((fname + \".sm\",data))\n",
    "    print(\"Going to loop through out var\")\n",
    "    # Loop through the out var\n",
    "    for (filename,content) in out:\n",
    "        # Open the file\n",
    "        with open(filename,\"w\") as out_file:\n",
    "            # Loop through the rows variable\n",
    "            for entry in content:\n",
    "                # Remove non unicode chars\n",
    "                entry = entry.encode('utf8', 'replace')\n",
    "                # Write the entry\n",
    "                out_file.write(entry)\n",
    "                # Add newline\n",
    "                out_file.write(\"\\n\")\n",
    "\n",
    "    # Path to the folder where the data files are\n",
    "    my_path = \"\"\n",
    "\n",
    "    # Loading the matrix from the three different files\n",
    "    my_space = Space.build(data = my_path + \"gutenberg_surface_3.sm\",\n",
    "                           rows = my_path + \"gutenberg_surface_3.rows\",\n",
    "                           cols = my_path + \"gutenberg_surface_3.cols\",\n",
    "                           format = \"sm\")\n",
    "    \n",
    "    return my_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to loop through the list\n",
      "Going to loop through the dictionary\n",
      "Going to loop through out var\n",
      "Progress...1000000\n",
      "Going to loop through the list\n",
      "Going to loop through the dictionary\n",
      "Going to loop through out var\n",
      "Progress...1000000\n",
      "Progress...2000000\n",
      "Progress...3000000\n",
      "Task 1 prepared\n"
     ]
    }
   ],
   "source": [
    "# START TASK 1-1\n",
    "\n",
    "space_ws1 = getSpace(1)\n",
    "\n",
    "# Transforming the semantic space using PPMI\n",
    "my_ppmi_space = space_ws1.apply(PpmiWeighting())\n",
    "\n",
    "ws1_pmi_svd50_Task_1_1  = my_ppmi_space.apply(Svd(50))\n",
    "# END TASK 1-1\n",
    "\n",
    "# START TASK 1-2to5\n",
    "space_ws3 = getSpace(3)\n",
    "# Transforming the semantic space using PPMI\n",
    "ppmi_space = space_ws3.apply(PpmiWeighting())\n",
    "\n",
    "ws3_Task_1_2 = space_ws3\n",
    "ws3_pmi_Task_1_3 = ppmi_space\n",
    "ws3_svd50_Task_1_4 = space_ws3.apply(Svd(50))\n",
    "ws3_pmi_svd50_Task_1_5 = ppmi_space.apply(Svd(50))\n",
    "\n",
    "# END TASK 1-2to5\n",
    "\n",
    "print(\"Task 1 prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity between man and woman\n",
      "window size 1, pmi, svd (50)\n",
      "('ws1_pmi_svd50_Task_1_1', 0.90197623371148516)\n",
      "window size 3\n",
      "('ws3_Task_1_2', 0.96862311057068851)\n",
      "window size 3, pmi\n",
      "('ws3_pmi_Task_1_3', 0.10418482739863029)\n",
      "window size 3, svd (50)\n",
      "('ws3_svd50_Task_1_4', 0.98125139575855458)\n",
      "window size 3, pmi, svd (50)\n",
      "('ws3_pmi_svd50_Task_1_5', 0.78366326238532669)\n"
     ]
    }
   ],
   "source": [
    "# START TASK 3    \n",
    "    \n",
    "# Comparing similarity between \"man\" and \"woman\"\n",
    "print(\"Calculating similarity between man and woman\")\n",
    "print(\"window size 1, pmi, svd (50)\")\n",
    "print(\"ws1_pmi_svd50_Task_1_1\", ws1_pmi_svd50_Task_1_1.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3\")\n",
    "print(\"ws3_Task_1_2\", ws3_Task_1_2.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi\")\n",
    "print(\"ws3_pmi_Task_1_3\", ws3_pmi_Task_1_3.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, svd (50)\")\n",
    "print(\"ws3_svd50_Task_1_4\", ws3_svd50_Task_1_4.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi, svd (50)\")\n",
    "print(\"ws3_pmi_svd50_Task_1_5\", ws3_pmi_svd50_Task_1_5.get_sim(\"man\", \"woman\", CosSimilarity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the 5 most similar words to 'car'\n",
      "window size 1, pmi, svd (50)\n",
      "('ws1_pmi_svd50_Task_1_1', [('car', 1.0), ('table', 0.85092128215815366), ('floor', 0.84634566222010721), ('chimney', 0.84395823872514431), ('dining', 0.84119473120215704)])\n",
      "window size 3\n",
      "('ws3_Task_1_2', [('car', 1.0), ('key', 0.93305619090166425), ('wall', 0.93037210089756406), ('street', 0.93034341502114104), ('sea', 0.92498355681372335)])\n",
      "window size 3, pmi\n",
      "('ws3_pmi_Task_1_3', [('car', 1.0), ('bicycles', 0.12817727288119293), ('popping', 0.12467056194780535), ('corpusants', 0.1071900718295629), ('stoical', 0.10669112901298419)])\n",
      "window size 3, svd (50)\n",
      "('ws3_svd50_Task_1_4', [('car', 1.0000000000000002), ('key', 0.98703744774949487), ('lawn', 0.98307380285577894), ('street', 0.97942380897437853), ('level', 0.97878276935629771)])\n",
      "window size 3, pmi, svd (50)\n",
      "('ws3_pmi_svd50_Task_1_5', [('car', 1.0), ('stick', 0.88739669849138947), ('window', 0.88371037927555984), ('lawn', 0.87665219281880669), ('corner', 0.87559867960864213)])\n"
     ]
    }
   ],
   "source": [
    "# Comparing the 5 most similar words to \"car\"\n",
    "\n",
    "print(\"Obtaining the 5 most similar words to 'car'\")\n",
    "print(\"window size 1, pmi, svd (50)\")\n",
    "print(\"ws1_pmi_svd50_Task_1_1\", ws1_pmi_svd50_Task_1_1.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3\")\n",
    "print(\"ws3_Task_1_2\", ws3_Task_1_2.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi\")\n",
    "print(\"ws3_pmi_Task_1_3\", ws3_pmi_Task_1_3.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, svd (50)\")\n",
    "print(\"ws3_svd50_Task_1_4\", ws3_svd50_Task_1_4.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi, svd (50)\")\n",
    "print(\"ws3_pmi_svd50_Task_1_5\", ws3_pmi_svd50_Task_1_5.get_neighbours(\"car\", 5, CosSimilarity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'synonyms.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01af672031ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"synonyms.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mword_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_tuple_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carlo/anaconda2/lib/python2.7/site-packages/dissect-0.1.0-py2.7.egg/composes/utils/io_utils.pyc\u001b[0m in \u001b[0;36mread_tuple_list\u001b[0;34m(data_file, fields)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfield_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'synonyms.txt'"
     ]
    }
   ],
   "source": [
    "my_path = \"\"\n",
    "# Comparing the similarity with \"gold standard\"\n",
    "fname = my_path + \"synonyms.txt\"\n",
    "# Load the pairs\n",
    "word_pairs = io_utils.read_tuple_list(fname, fields=[0,1])\n",
    "# Load the score\n",
    "gold = io_utils.read_list(fname, field=2)\n",
    "# Predict similarity\n",
    "\n",
    "def printSimilarity(model):\n",
    "\n",
    "    print(\"Comparing similarity with 'gold standard'\")\n",
    "    print(\"window size 1, pmi, svd (50)\")\n",
    "    predicted_ppmi_svd = [round(sim,2) for sim in model.get_sims(word_pairs, CosSimilarity())]\n",
    "    print (\"Pairs:\",word_pairs)\n",
    "    print (\"Gold scores\",gold)\n",
    "    print (\"\\n PPMI and SVD matrix:\")\n",
    "    print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "    print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "    print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "models = [ws1_pmi_svd50_Task_1_1, ws3_Task_1_2, ws3_pmi_Task_1_3, ws3_svd50_Task_1_4, ws3_pmi_svd50_Task_1_5] \n",
    "\n",
    "map(lambda model: printSimilarity(model), models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
