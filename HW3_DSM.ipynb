{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 3: Distributional semantic models.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Obtain co-occurrence vector representations with the followin properties:\n",
    "- window size 1, pmi, svd (50)\n",
    "- window size 3, no modifications\n",
    "- window size 3, pmi, no svd\n",
    "- window size 3, no pmi, svd (50)\n",
    "- window size 3, pmi, svd (50)\n",
    "\n",
    "2) Obtain word2vec embeddings with the following properties\n",
    "- window size 1, 50 dimensions\n",
    "- window size 1, 200 dimensions\n",
    "- window size 3, 50 dimensions\n",
    "- window size 3, 200 dimensions\n",
    "- window size 5, 50 dimensions\n",
    "\n",
    "3) Compare the performance of the 10 representations in 1 and 2 on the following tasks:\n",
    "- similarity between \"man\" and \"woman\"\n",
    "- the 5 most similar words to \"car\"\n",
    "- for DISSECT representations , correlation with gold standard\n",
    "- for Word2Vec, the similarity between \"queen\" and \"king + woman - man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import section\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import spatial\n",
    "\n",
    "# Dissect\n",
    "from composes.semantic_space.space import Space\n",
    "from composes.utils import io_utils\n",
    "from composes.transformation.scaling.ppmi_weighting import PpmiWeighting\n",
    "from composes.transformation.dim_reduction.svd import Svd\n",
    "from composes.similarity.cos import CosSimilarity\n",
    "from composes.utils import scoring_utils\n",
    "\n",
    "# Gensim\n",
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the corpus\n",
    "corpus = gutenberg.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSpace(windowSize):\n",
    "    ## generate the raw co-occurrence count within a window of windowSize\n",
    "    cooc = BigramCollocationFinder.from_words(corpus,window_size=windowSize + 1).ngram_fd.items()\n",
    "\n",
    "    ## convert the list of collocates in a dictionary\n",
    "\n",
    "    # Initialize the dict\n",
    "    cooc_dict = {}\n",
    "    print(\"Going to loop through the list\")\n",
    "    # Loop through the list\n",
    "    for pair,freq in cooc:\n",
    "        # Check and initialie the variables\n",
    "        word1,word2 = pair\n",
    "        # Check if entries for the words exist\n",
    "        # If not, create them\n",
    "        if word1 not in cooc_dict:\n",
    "            cooc_dict[word1]={}\n",
    "\n",
    "        if word2 not in cooc_dict:\n",
    "            cooc_dict[word2]={}\n",
    "\n",
    "        # Check if entries for the particular combination exists\n",
    "        # If not, initialize them\n",
    "        if word2 not in cooc_dict[word1]:\n",
    "            cooc_dict[word1][word2]=0\n",
    "        if word1 not in cooc_dict[word2]:\n",
    "            cooc_dict[word2][word1]=0\n",
    "        # Update the dict variables\n",
    "        cooc_dict[word1][word2]+=freq\n",
    "        cooc_dict[word2][word1]+=freq\n",
    "\n",
    "    ## Generate the row, col and data variables for the DISSECT\n",
    "\n",
    "    # Initialize the variables\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "    print(\"Going to loop through the dictionary\")\n",
    "    # Loop through the dictionary\n",
    "    for word_1 in cooc_dict:\n",
    "        # Add an entry to the rows variable\n",
    "        # there should be no duplications, but we check anyway\n",
    "        if word_1 not in rows:\n",
    "            rows.append(word_1)\n",
    "        # Loop through the entries in the dict\n",
    "        for word_2 in cooc_dict[word_1]:\n",
    "            # Add an entry in the cols, if it's not already added\n",
    "            if word_2 not in cols:\n",
    "                cols.append(word_2)\n",
    "            # Add the value to the data\n",
    "            data.append(word_1 + \" \" + word_2 + \" \" + str(cooc_dict[word_1][word_2]))\n",
    "\n",
    "    ## Output the row,col,data to files\n",
    "\n",
    "    # Define the base name\n",
    "    fname = \"gutenberg_surface_3\"\n",
    "\n",
    "    # Generate tuples of fname data for the files\n",
    "    out = []\n",
    "    out.append((fname + \".rows\",rows))\n",
    "    out.append((fname + \".cols\",cols))\n",
    "    out.append((fname + \".sm\",data))\n",
    "    print(\"Going to loop through out var\")\n",
    "    # Loop through the out var\n",
    "    for (filename,content) in out:\n",
    "        # Open the file\n",
    "        with open(filename,\"w\") as out_file:\n",
    "            # Loop through the rows variable\n",
    "            for entry in content:\n",
    "                # Remove non unicode chars\n",
    "                entry = entry.encode('utf8', 'replace')\n",
    "                # Write the entry\n",
    "                out_file.write(entry)\n",
    "                # Add newline\n",
    "                out_file.write(\"\\n\")\n",
    "\n",
    "    # Path to the folder where the data files are\n",
    "    my_path = \"\"\n",
    "\n",
    "    # Loading the matrix from the three different files\n",
    "    my_space = Space.build(data = my_path + \"gutenberg_surface_3.sm\",\n",
    "                           rows = my_path + \"gutenberg_surface_3.rows\",\n",
    "                           cols = my_path + \"gutenberg_surface_3.cols\",\n",
    "                           format = \"sm\")\n",
    "    \n",
    "    return my_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  co-occurrence vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to loop through the list\n",
      "Going to loop through the dictionary\n",
      "Going to loop through out var\n",
      "Progress...1000000\n",
      "Going to loop through the list\n",
      "Going to loop through the dictionary\n",
      "Going to loop through out var\n",
      "Progress...1000000\n",
      "Progress...2000000\n",
      "Progress...3000000\n",
      "Task 1 prepared\n"
     ]
    }
   ],
   "source": [
    "# START TASK 1-1\n",
    "# window size 1, pmi, svd (50)\n",
    "space_ws1 = getSpace(1)\n",
    "\n",
    "# Transforming the semantic space using PPMI\n",
    "my_ppmi_space = space_ws1.apply(PpmiWeighting())\n",
    "\n",
    "ws1_pmi_svd50_Task_1_1  = my_ppmi_space.apply(Svd(50))\n",
    "# END TASK 1-1\n",
    "\n",
    "# START TASK 1-2to5\n",
    "\n",
    "space_ws3 = getSpace(3)\n",
    "\n",
    "# Transforming the semantic space using PPMI\n",
    "ppmi_space = space_ws3.apply(PpmiWeighting())\n",
    "\n",
    "# window size 3, no modifications\n",
    "ws3_Task_1_2 = space_ws3\n",
    "\n",
    "# window size 3, pmi, no svd\n",
    "ws3_pmi_Task_1_3 = ppmi_space\n",
    "\n",
    "# window size 3, no pmi, svd (50)\n",
    "ws3_svd50_Task_1_4 = space_ws3.apply(Svd(50))\n",
    "\n",
    "# window size 3, pmi, svd (50)\n",
    "ws3_pmi_svd50_Task_1_5 = ppmi_space.apply(Svd(50))\n",
    "\n",
    "# END TASK 1-2to5\n",
    "\n",
    "print(\"Task 1 prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Embedding...\n",
      "Embedding...\n",
      "Embedding...\n"
     ]
    }
   ],
   "source": [
    "## Load the corpus\n",
    "corpus = gutenberg.sents()\n",
    "\n",
    "def word_2_vec_models(corpus, sizes, windows):\n",
    "    models = {}\n",
    "    for size, window in zip(sizes,windows):\n",
    "        print ('Embedding...')\n",
    "        model = Word2Vec(corpus, size=size, window=window, workers=4, compute_loss=True)\n",
    "        model.train(corpus, total_examples=len(corpus), epochs=10)\n",
    "        models['w2v_size_' + str(size) + '_window_' + str(window)] = model\n",
    "    return models\n",
    "\n",
    "sizes = [50, 200, 50, 200, 50]\n",
    "windows = [1, 1, 3, 3, 5]\n",
    "\n",
    "w2v_models = word_2_vec_models(corpus, sizes, windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# print loss of models\n",
    "for key, model in w2v_models.iteritems():\n",
    "    print model.get_latest_training_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between 'man' and 'woman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity between man and woman\n",
      "Co-occurrence vector:\n",
      "\n",
      "('window size 1, pmi, svd (50)', 0.90197623371148516)\n",
      "('window size 3, no modifications', 0.96862311057068851)\n",
      "('window size 3, pmi, no svd', 0.10418482739863029)\n",
      "('window size 3, no pmi, svd (50)', 0.98125139575855458)\n",
      "('window size 3, pmi, svd (50)', 0.78366326238532669)\n",
      "\n",
      "Word vectors:\n",
      "\n",
      "('w2v_size_50_window_3', 0.81365877302885359)\n",
      "('w2v_size_50_window_5', 0.70289617991937781)\n",
      "('w2v_size_200_window_1', 0.67420059581009006)\n",
      "('w2v_size_200_window_3', 0.64307042451720342)\n",
      "('w2v_size_50_window_1', 0.86046230344278984)\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating similarity between man and woman\")\n",
    "\n",
    "print ('Co-occurrence vector:\\n')\n",
    "\n",
    "print(\"window size 1, pmi, svd (50)\", ws1_pmi_svd50_Task_1_1.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "print(\"window size 3, no modifications\", ws3_Task_1_2.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "print(\"window size 3, pmi, no svd\", ws3_pmi_Task_1_3.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "print(\"window size 3, no pmi, svd (50)\", ws3_svd50_Task_1_4.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "print(\"window size 3, pmi, svd (50)\", ws3_pmi_svd50_Task_1_5.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "# Word vectors\n",
    "print('\\nWord vectors:\\n')\n",
    "\n",
    "performance_df = {k: [] for k in w2v_models}\n",
    "\n",
    "for key, model in w2v_models.iteritems():\n",
    "    print(key, model.wv.similarity(w1=\"man\", w2=\"woman\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 most similar words to \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining the 5 most similar words to 'car'\n",
      "co-occurence vector:\n",
      "\n",
      "('window size 1, pmi, svd (50)', [('car', 1.0), ('table', 0.85092128215815366), ('floor', 0.84634566222010721), ('chimney', 0.84395823872514431), ('dining', 0.84119473120215704)])\n",
      "('window size 3, no modifications', [('car', 1.0), ('key', 0.93305619090166425), ('wall', 0.93037210089756406), ('street', 0.93034341502114104), ('sea', 0.92498355681372335)])\n",
      "('window size 3, pmi, no svd', [('car', 1.0), ('bicycles', 0.12817727288119293), ('popping', 0.12467056194780535), ('corpusants', 0.1071900718295629), ('stoical', 0.10669112901298419)])\n",
      "('window size 3, no pmi, svd (50)', [('car', 1.0000000000000002), ('key', 0.98703744774949487), ('lawn', 0.98307380285577894), ('street', 0.97942380897437853), ('level', 0.97878276935629771)])\n",
      "('window size 3, pmi, svd (50)', [('car', 1.0), ('stick', 0.88739669849138947), ('window', 0.88371037927555984), ('lawn', 0.87665219281880669), ('corner', 0.87559867960864213)])\n",
      "\n",
      "Word vectors:\n",
      "\n",
      "('w2v_size_50_window_3', [(u'lane', 0.8670400381088257), (u'deck', 0.8182364106178284), (u'bushes', 0.8153541088104248), (u'road', 0.8137567043304443), (u'cabin', 0.810718297958374)])\n",
      "('w2v_size_50_window_5', [(u'coach', 0.8447237014770508), (u'lane', 0.8435376882553101), (u'boat', 0.8208440542221069), (u'window', 0.7984238862991333), (u'rope', 0.7976034879684448)])\n",
      "('w2v_size_200_window_1', [(u'shutters', 0.7156025171279907), (u'bushes', 0.7109667062759399), (u'forwards', 0.7003048658370972), (u'buckets', 0.6960436105728149), (u'lane', 0.6924281716346741)])\n",
      "('w2v_size_200_window_3', [(u'lane', 0.7617424726486206), (u'bushes', 0.7384689450263977), (u'coach', 0.7293598651885986), (u'floor', 0.7284582853317261), (u'road', 0.7159005403518677)])\n",
      "('w2v_size_50_window_1', [(u'bushes', 0.7632997035980225), (u'shutters', 0.7552960515022278), (u'bows', 0.7476483583450317), (u'lane', 0.7450878620147705), (u'deck', 0.7450588941574097)])\n"
     ]
    }
   ],
   "source": [
    "print(\"Obtaining the 5 most similar words to 'car'\")\n",
    "print(\"co-occurence vector:\\n\")\n",
    "\n",
    "print(\"window size 1, pmi, svd (50)\", ws1_pmi_svd50_Task_1_1.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "print(\"window size 3, no modifications\", ws3_Task_1_2.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "print(\"window size 3, pmi, no svd\", ws3_pmi_Task_1_3.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "print(\"window size 3, no pmi, svd (50)\", ws3_svd50_Task_1_4.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "print(\"window size 3, pmi, svd (50)\", ws3_pmi_svd50_Task_1_5.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "# word vectors\n",
    "print (\"\\nWord vectors:\\n\")\n",
    "performance_df = {k: [] for k in w2v_models}\n",
    "\n",
    "for key, model in w2v_models.iteritems():\n",
    "    print (key, model.wv.most_similar(positive=\"car\", topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for DISSECT representations , correlation with gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing similarity with 'gold standard'\n",
      "window size 1, pmi, svd (50)\n",
      "('Pairs:', [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')])\n",
      "('Gold scores', ['10', '8', '5', '10', '7', '2', '10', '3', '7'])\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "('Predicted scores', [0.81, 0.52, 0.51, 0.35, 0.42, 0.24, 0.28, 0.12, 0.33])\n",
      "('Spearman correlation:', 0.49372878784007418)\n",
      "('Pearson correlation:', 0.5248507614047011)\n",
      "Comparing similarity with 'gold standard'\n",
      "window size 1, pmi, svd (50)\n",
      "('Pairs:', [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')])\n",
      "('Gold scores', ['10', '8', '5', '10', '7', '2', '10', '3', '7'])\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "('Predicted scores', [0.84, 0.83, 0.71, 0.56, 0.3, 0.34, 0.41, 0.66, 0.82])\n",
      "('Spearman correlation:', 0.28942722045797453)\n",
      "('Pearson correlation:', 0.21350087833818576)\n",
      "Comparing similarity with 'gold standard'\n",
      "window size 1, pmi, svd (50)\n",
      "('Pairs:', [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')])\n",
      "('Gold scores', ['10', '8', '5', '10', '7', '2', '10', '3', '7'])\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "('Predicted scores', [0.03, 0.03, 0.02, 0.03, 0.0, 0.0, 0.0, 0.01, 0.02])\n",
      "('Spearman correlation:', 0.48237644517632489)\n",
      "('Pearson correlation:', 0.45207905986890301)\n",
      "Comparing similarity with 'gold standard'\n",
      "window size 1, pmi, svd (50)\n",
      "('Pairs:', [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')])\n",
      "('Gold scores', ['10', '8', '5', '10', '7', '2', '10', '3', '7'])\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "('Predicted scores', [0.93, 0.91, 0.82, 0.63, 0.54, 0.52, 0.59, 0.74, 0.91])\n",
      "('Spearman correlation:', 0.33765617369326306)\n",
      "('Pearson correlation:', 0.22142573694642415)\n",
      "Comparing similarity with 'gold standard'\n",
      "window size 1, pmi, svd (50)\n",
      "('Pairs:', [('awful', 'terrible'), ('awful', 'great'), ('awful', 'fast'), ('chop', 'cut'), ('chop', 'bake'), ('chop', 'smile'), ('material', 'fabric'), ('material', 'car'), ('material', 'stone')])\n",
      "('Gold scores', ['10', '8', '5', '10', '7', '2', '10', '3', '7'])\n",
      "\n",
      " PPMI and SVD matrix:\n",
      "('Predicted scores', [0.85, 0.48, 0.49, 0.42, 0.33, 0.21, 0.31, 0.32, 0.33])\n",
      "('Spearman correlation:', 0.41886462053088325)\n",
      "('Pearson correlation:', 0.51884655677607738)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_path = \"\"\n",
    "# Comparing the similarity with \"gold standard\"\n",
    "fname = my_path + \"synonyms.txt\"\n",
    "# Load the pairs\n",
    "word_pairs = io_utils.read_tuple_list(fname, fields=[0,1])\n",
    "# Load the score\n",
    "gold = io_utils.read_list(fname, field=2)\n",
    "# Predict similarity\n",
    "\n",
    "def printSimilarity(model):\n",
    "\n",
    "    print(\"Comparing similarity with 'gold standard'\")\n",
    "    print(\"window size 1, pmi, svd (50)\")\n",
    "    predicted_ppmi_svd = [round(sim,2) for sim in model.get_sims(word_pairs, CosSimilarity())]\n",
    "    print (\"Pairs:\",word_pairs)\n",
    "    print (\"Gold scores\",gold)\n",
    "    print (\"\\n PPMI and SVD matrix:\")\n",
    "    print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "    print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "    print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "models = [ws1_pmi_svd50_Task_1_1, ws3_Task_1_2, ws3_pmi_Task_1_3, ws3_svd50_Task_1_4, ws3_pmi_svd50_Task_1_5] \n",
    "\n",
    "map(lambda model: printSimilarity(model), models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for Word2Vec, the similarity between \"queen\" and \"king + woman - man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w2v_size_50_window_3', 0.45716172456741333)\n",
      "('w2v_size_50_window_5', 0.64034128189086914)\n",
      "('w2v_size_200_window_1', 0.44070711731910706)\n",
      "('w2v_size_200_window_3', 0.41038885712623596)\n",
      "('w2v_size_50_window_1', 0.46846812963485718)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/home/carlo/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "performance_df = {k: [] for k in w2v_models}\n",
    "\n",
    "for key, model in w2v_models.iteritems():    \n",
    "    custom_queen = np.add(np.subtract(model[\"king\"],model[\"man\"]),model[\"woman\"])\n",
    "    print (key, 1 - spatial.distance.cosine(custom_queen,model[\"queen\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
