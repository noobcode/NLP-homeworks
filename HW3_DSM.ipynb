{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 3: Distributional semantic models.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Obtain co-occurrence vector representations with the followin properties:\n",
    "- window size 1, pmi, svd (50)\n",
    "- window size 3, no modifications\n",
    "- window size 3, pmi, no svd\n",
    "- window size 3, no pmi, svd (50)\n",
    "- window size 3, pmi, svd (50)\n",
    "\n",
    "2) Obtain word2vec embeddings with the following properties\n",
    "- window size 1, 50 dimensions\n",
    "- window size 1, 200 dimensions\n",
    "- window size 3, 50 dimensions\n",
    "- window size 3, 200 dimensions\n",
    "- window size 5, 50 dimensions\n",
    "\n",
    "3) Compare the performance of the 10 representations in 1 and 2 on the following tasks:\n",
    "- similarity between \"man\" and \"woman\"\n",
    "- the 5 most similar words to \"car\"\n",
    "- for DISSECT representations , correlation with gold standard\n",
    "- for Word2Vec, the similarity between \"queen\" and \"king + woman - man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (space.py, line 206)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-f2e8a9f36215>\"\u001b[0m, line \u001b[1;32m13\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from composes.semantic_space.space import Space\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m969\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m954\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m896\u001b[0m, in \u001b[1;35m_find_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1139\u001b[0m, in \u001b[1;35mfind_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1115\u001b[0m, in \u001b[1;35m_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1096\u001b[0m, in \u001b[1;35m_legacy_get_spec\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m444\u001b[0m, in \u001b[1;35mspec_from_loader\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap_external>\"\u001b[0;36m, line \u001b[0;32m533\u001b[0;36m, in \u001b[0;35mspec_from_file_location\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.5/dist-packages/dissect-0.1.0-py3.5.egg/composes/semantic_space/space.py\"\u001b[0;36m, line \u001b[0;32m206\u001b[0m\n\u001b[0;31m    print \"Row string %s not found, returning 0.0\" % (word1)\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import section\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import spatial\n",
    "\n",
    "# Dissect\n",
    "from composes.semantic_space.space import Space\n",
    "from composes.utils import io_utils\n",
    "from composes.transformation.scaling.ppmi_weighting import PpmiWeighting\n",
    "from composes.transformation.dim_reduction.svd import Svd\n",
    "from composes.similarity.cos import CosSimilarity\n",
    "from composes.utils import scoring_utils\n",
    "\n",
    "# Gensim\n",
    "import gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Load the corpus\n",
    "corpus = gutenberg.words()\n",
    "\n",
    "\n",
    "def getSpace(windowSize):\n",
    "    ## generate the raw co-occurrence count within a window of 1\n",
    "    cooc = BigramCollocationFinder.from_words(corpus,window_size=windowSize + 1).ngram_fd.items()\n",
    "\n",
    "    ## convert the list of collocates in a dictionary\n",
    "\n",
    "    # Initialize the dict\n",
    "    cooc_dict = {}\n",
    "\n",
    "    # Loop through the list\n",
    "    for pair,freq in cooc:\n",
    "        # Check and initialie the variables\n",
    "        word1,word2 = pair\n",
    "        # Check if entries for the words exist\n",
    "        # If not, create them\n",
    "        if word1 not in cooc_dict:\n",
    "            cooc_dict[word1]={}\n",
    "\n",
    "        if word2 not in cooc_dict:\n",
    "            cooc_dict[word2]={}\n",
    "\n",
    "        # Check if entries for the particular combination exists\n",
    "        # If not, initialize them\n",
    "        if word2 not in cooc_dict[word1]:\n",
    "            cooc_dict[word1][word2]=0\n",
    "        if word1 not in cooc_dict[word2]:\n",
    "            cooc_dict[word2][word1]=0\n",
    "        # Update the dict variables\n",
    "        cooc_dict[word1][word2]+=freq\n",
    "        cooc_dict[word2][word1]+=freq\n",
    "\n",
    "    ## Generate the row, col and data variables for the DISSECT\n",
    "\n",
    "    # Initialize the variables\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    # Loop through the dictionary\n",
    "    for word_1 in cooc_dict:\n",
    "        # Add an entry to the rows variable\n",
    "        # there should be no duplications, but we check anyway\n",
    "        if word_1 not in rows:\n",
    "            rows.append(word_1)\n",
    "        # Loop through the entries in the dict\n",
    "        for word_2 in cooc_dict[word_1]:\n",
    "            # Add an entry in the cols, if it's not already added\n",
    "            if word_2 not in cols:\n",
    "                cols.append(word_2)\n",
    "            # Add the value to the data\n",
    "            data.append(word_1 + \" \" + word_2 + \" \" + str(cooc_dict[word_1][word_2]))\n",
    "\n",
    "    ## Output the row,col,data to files\n",
    "\n",
    "    # Define the base name\n",
    "    fname = \"gutenberg_surface_3\"\n",
    "\n",
    "    # Generate tuples of fname data for the files\n",
    "    out = []\n",
    "    out.append((fname + \".rows\",rows))\n",
    "    out.append((fname + \".cols\",cols))\n",
    "    out.append((fname + \".sm\",data))\n",
    "\n",
    "    # Loop through the out var\n",
    "    for (filename,content) in out:\n",
    "        # Open the file\n",
    "        with open(filename,\"w\") as out_file:\n",
    "            # Loop through the rows variable\n",
    "            for entry in content:\n",
    "                # Remove non unicode chars\n",
    "                entry = entry.encode('utf8', 'replace')\n",
    "                # Write the entry\n",
    "                out_file.write(entry)\n",
    "                # Add newline\n",
    "                out_file.write(\"\\n\")\n",
    "\n",
    "    # Path to the folder where the data files are\n",
    "    my_path = \"\"\n",
    "\n",
    "    # Loading the matrix from the three different files\n",
    "    my_space = Space.build(data = my_path + \"gutenberg_surface_3.sm\",\n",
    "                           rows = my_path + \"gutenberg_surface_3.rows\",\n",
    "                           cols = my_path + \"gutenberg_surface_3.cols\",\n",
    "                           format = \"sm\")\n",
    "    \n",
    "    return my_space\n",
    "\n",
    "# START TASK 1-1\n",
    "\n",
    "space_ws1 = getSpace(1)\n",
    "\n",
    "# Transforming the semantic space using PPMI\n",
    "my_ppmi_space = space_ws1.apply(PpmiWeighting())\n",
    "\n",
    "ws1_pmi_svd50_Task_1_1  = my_ppmi_space.apply(Svd(50))\n",
    "# END TASK 1-1\n",
    "\n",
    "# START TASK 1-2to5\n",
    "space_ws3 = getSpace(3)\n",
    "# Transforming the semantic space using PPMI\n",
    "ppmi_space = space_ws3.apply(PpmiWeighting())\n",
    "\n",
    "ws3_Task_1_2 = space_ws3\n",
    "ws3_pmi_Task_1_3 = ppmi_space\n",
    "ws3_svd50_Task_1_4 = space_ws3.apply(Svd(50))\n",
    "ws3_pmi_svd50_Task_1_5 = ppmi_space.apply(Svd(50))\n",
    "\n",
    "# END TASK 1-2to5\n",
    " \n",
    "    \n",
    "# START TASK 3    \n",
    "    \n",
    "# Comparing similarity between \"man\" and \"woman\"\n",
    "\n",
    "print(\"Calculating similarity between man and woman\")\n",
    "print(\"window size 1, pmi, svd (50)\")\n",
    "print(\"ws1_pmi_svd50_Task_1_1\", ws1_pmi_svd50_Task_1_1.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3\")\n",
    "print(\"ws3_Task_1_2\", ws3_Task_1_2.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi\")\n",
    "print(\"ws3_pmi_Task_1_3\", ws3_pmi_Task_1_3.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, svd (50)\")\n",
    "print(\"ws3_svd50_Task_1_4\", ws3_svd50_Task_1_4.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi, svd (50)\")\n",
    "print(\"ws3_pmi_svd50_Task_1_5\", ws3_pmi_svd50_Task_1_5.get_sim(\"man\", \"woman\", CosSimilarity()))\n",
    "\n",
    "\n",
    "# Comparing the 5 most similar words to \"car\"\n",
    "\n",
    "print(\"Obtaining the 5 most similar words to 'car'\")\n",
    "print(\"window size 1, pmi, svd (50)\")\n",
    "print(\"ws1_pmi_svd50_Task_1_1\", ws1_pmi_svd50_Task_1_1.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3\")\n",
    "print(\"ws3_Task_1_2\", ws3_Task_1_2.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi\")\n",
    "print(\"ws3_pmi_Task_1_3\", ws3_pmi_Task_1_3.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, svd (50)\")\n",
    "print(\"ws3_svd50_Task_1_4\", ws3_svd50_Task_1_4.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "print(\"window size 3, pmi, svd (50)\")\n",
    "print(\"ws3_pmi_svd50_Task_1_5\", ws3_pmi_svd50_Task_1_5.get_neighbours(\"car\", 5, CosSimilarity()))\n",
    "\n",
    "# Comparing the similarity with \"gold standard\"\n",
    "fname = my_path + \"synonyms.txt\"\n",
    "# Load the pairs\n",
    "word_pairs = io_utils.read_tuple_list(fname, fields=[0,1])\n",
    "# Load the score\n",
    "gold = io_utils.read_list(fname, field=2)\n",
    "# Predict similarity\n",
    "\n",
    "print(\"Comparing similarity with 'gold standard'\")\n",
    "print(\"window size 1, pmi, svd (50)\")\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in ws1_pmi_svd50_Task_1_1.get_sims(word_pairs, CosSimilarity())]\n",
    "print (\"Pairs:\",word_pairs)\n",
    "print (\"Gold scores\",gold)\n",
    "print (\"\\n PPMI and SVD matrix:\")\n",
    "print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "print(\"window size 3\")\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in ws3_Task_1_2.get_sims(word_pairs, CosSimilarity())]\n",
    "print (\"Pairs:\",word_pairs)\n",
    "print (\"Gold scores\",gold)\n",
    "print (\"\\n PPMI and SVD matrix:\")\n",
    "print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "print(\"window size 3, pmi\")\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in ws3_pmi_Task_1_3.get_sims(word_pairs, CosSimilarity())]\n",
    "print (\"Pairs:\",word_pairs)\n",
    "print (\"Gold scores\",gold)\n",
    "print (\"\\n PPMI and SVD matrix:\")\n",
    "print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "print(\"window size 3, svd (50)\")\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in ws3_svd50_Task_1_4.get_sims(word_pairs, CosSimilarity())]\n",
    "print (\"Pairs:\",word_pairs)\n",
    "print (\"Gold scores\",gold)\n",
    "print (\"\\n PPMI and SVD matrix:\")\n",
    "print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n",
    "print(\"window size 3, pmi, svd (50)\")\n",
    "predicted_ppmi_svd = [round(sim,2) for sim in ws3_pmi_svd50_Task_1_5.get_sims(word_pairs, CosSimilarity())]\n",
    "print (\"Pairs:\",word_pairs)\n",
    "print (\"Gold scores\",gold)\n",
    "print (\"\\n PPMI and SVD matrix:\")\n",
    "print (\"Predicted scores\",predicted_ppmi_svd)\n",
    "print (\"Spearman correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"spearman\"))\n",
    "print (\"Pearson correlation:\",scoring_utils.score(gold, predicted_ppmi_svd, \"pearson\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
