{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to NLP course (2017-2018).\n",
    "\n",
    "Homework 2.1: Markov Models. Hidden Markov Models and Part of Speech Tagging.\n",
    "\n",
    "Objectives:\n",
    "\n",
    "1) Create a tri-gram model for generating pseudo-Trump sentences \n",
    "- load the corpus, tokenize it and obtain list of trigrams \n",
    "- define a function that obtains the counts of the \"model\" \n",
    "- define a function that generates a pseudo-sentence \n",
    "- when generating a sentence, make sure that your sentence fulfils the following requirements\n",
    "    - it is at least 5 words long\n",
    "    - the last token of the pseudo-sentence is a \".\", \"!\", or \"?\"\n",
    "    - it does not contain any other \".\", \"!\", \"?\" tokens other than the final one\n",
    "- print 5 pseudo-sentences\n",
    "\n",
    "2) Use the built-in n-gram HMM models in nltk to tag a corpus \n",
    "- load the brown corpus\n",
    "- split each category in the corpus to test and train\n",
    "- for each category in the corpus, train on the train set and evaluate on the test set the following taggers:\n",
    "    - default\n",
    "    - affix\n",
    "    - unigram\n",
    "    - bigram\n",
    "    - trigram\n",
    "    \n",
    "    Each tagger should have backoff configured on the previous tagger.\n",
    "    \n",
    "    Print the results in a table.\n",
    "    \n",
    "    \n",
    "- repeat the previous experiment using universal tagset. Print the results in a table.\n",
    "- cross evaluate between different genres (train on one category, evaluate on all the other categories). Print and compare the results\n",
    "- Only for the \"news\" portion of the corpus, compare\n",
    "    - the best berforming tagger (with backoff)\n",
    "    - the naive bayes tagger\n",
    "    \n",
    "    Compare the accuracy as well as the execution time.\n",
    "    \n",
    "    Use both the universal tagset and the full tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import codecs\n",
    "import codecs\n",
    "\n",
    "# Import taggers\n",
    "from nltk import DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk import ClassifierBasedPOSTagger\n",
    "\n",
    "# Import the brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Homework 2 part 1\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "\n",
    "def hw2_part1():\n",
    "    \n",
    "    # Trump speeches file location\n",
    "    fname = \"speeches.txt\"\n",
    "    # Read the corpus\n",
    "    raw_corpus = codecs.open(fname,'r','utf8').read()\n",
    "    \n",
    "    # Tokenize the corpus\n",
    "    corpus = nltk.word_tokenize(raw_corpus)\n",
    "\n",
    "    # Generate list of trigrams\n",
    "    \n",
    "    \n",
    "    # Initialize the \"markov model\"\n",
    "    # Preferably, you should define a function (or an object)\n",
    "    # which \"trains\" the model. You should just invoke the function here.\n",
    "\n",
    "    \n",
    "    # Fill in all the counts \n",
    "\n",
    "\n",
    "    # Generate a sentence\n",
    "    # Preferably you should define a function (or an object)\n",
    "    # which \"generates\" a sentence following the requirements (min length, ending with punctuation, etc)\n",
    "    # You should just invoke the code here\n",
    "    \n",
    "    \n",
    "    # Print the sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2 part 2\n",
    "\n",
    "# Function that splits a corpus in train and test\n",
    "def split_train_test(corpus,test_size=500):\n",
    "    return corpus[test_size:], corpus[:test_size]\n",
    "\n",
    "def split_each_category_in_train_and_test(corpus, fraction_ts, tagset=None):\n",
    "    train_sets = {} # dictionary of training sets, one key for each category\n",
    "    test_sets = {}  # dictionary of test sets, one key for each category\n",
    "    for category in corpus.categories():\n",
    "        corpus_tsents = corpus.tagged_sents(categories=category, tagset=tagset)\n",
    "        tsents_train, tsents_test = split_train_test(corpus_tsents, test_size=int(fraction_ts * len(corpus_tsents)))\n",
    "        # add training and test set to the corresponding dictionary\n",
    "        train_sets[category] = tsents_train\n",
    "        test_sets[category] = tsents_test\n",
    "        \n",
    "    return train_sets, test_sets\n",
    "\n",
    "def get_most_common_tag(train_set):\n",
    "    flat_list = [item for sublist in train_set for item in sublist]\n",
    "    tags = [tag for (word, tag) in flat_list]\n",
    "    # Get the most frequent tag in the training set\n",
    "    most_frequent_tag = nltk.FreqDist(tags).max()\n",
    "    return most_frequent_tag\n",
    "\n",
    "# For each category, train and evaluate taggers. Use backoff.\n",
    "def train_and_evaluate_taggers(taggers_list, corpus, train_sets, test_sets):\n",
    "    # dictionary to keep the results of each tagger on each category\n",
    "    benchmarks_df = {t.__name__: [] for t in taggers_list}\n",
    "    # dictionary to keep models trained on each category\n",
    "    models_df = {t.__name__: [] for t in taggers_list}\n",
    "    \n",
    "    for i, category in enumerate(brown.categories()):\n",
    "        for tagger in taggers_list:\n",
    "            if tagger == DefaultTagger:\n",
    "                most_common_tag = get_most_common_tag(train_sets[category])\n",
    "                # the default tagger does not have a backoff tagger\n",
    "                tagger_model = tagger(most_common_tag)\n",
    "            else:\n",
    "                # each tagger takes as backoff tagger the previous tagger\n",
    "                tagger_model = tagger(train_sets[category], backoff=tagger_model)\n",
    "            # evaluate tagger\n",
    "            accuracy = round(tagger_model.evaluate(test_sets[category]),4) * 100\n",
    "            # append to dictionaries accuracy and model\n",
    "            benchmarks_df[tagger.__name__].append(accuracy)\n",
    "            models_df[tagger.__name__].append(tagger_model)\n",
    "    \n",
    "    # returns the statistics for all taggers and all categories\n",
    "    benchmarks_df = pd.DataFrame(benchmarks_df, index=brown.categories())\n",
    "    models_df = pd.DataFrame(models_df, index=brown.categories())\n",
    "    return benchmarks_df, models_df\n",
    "\n",
    "# Dummy function\n",
    "# Extend and rework\n",
    "def hw2_part2():\n",
    "    # list of taggers to benchmark\n",
    "    taggers = [DefaultTagger, AffixTagger, UnigramTagger, BigramTagger, TrigramTagger]\n",
    "    fraction_ts = 0.2 # faction test set\n",
    "    \n",
    "    ### BROWN TAGSET   \n",
    "    # Split each category in the brown corpus into train and test\n",
    "    train_sets_full, test_sets_full = split_each_category_in_train_and_test(brown, fraction_ts)\n",
    "    brown_tagset_df, models_df_full = train_and_evaluate_taggers(taggers, brown, train_sets_full, test_sets_full)\n",
    "    print (brown_tagset_df)\n",
    "    \n",
    "    ### UNIVERSAL TAGSET\n",
    "    # Split each category in the brown corpus into train and test using tagset='universal'\n",
    "    train_sets_uni, test_sets_uni = split_each_category_in_train_and_test(brown, fraction_ts, tagset='universal')\n",
    "    universal_tagset_df, models_df_uni = train_and_evaluate_taggers(taggers, brown, train_sets_uni, test_sets_uni)\n",
    "    print (universal_tagset_df)   \n",
    "    \n",
    "    ### NB classifier\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (full tagset)  \n",
    "    bigram_tagger = models_df_full.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime = time.time() - toc\n",
    "    print \"BigramTagger on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (full tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_full['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (full tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_full['news']) * 100\n",
    "    runtime_full = round(time.time() - tic, 2)\n",
    "    print \"Naive Bayes on news, (full tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_full\n",
    "    \n",
    "    # Print the performance of the best performing n-gram tagger and the runtime (universal tagset)\n",
    "    bigram_tagger = models_df_uni.loc['news', 'BigramTagger']\n",
    "    tic = time.time()\n",
    "    accuracy = bigram_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime = time.time() - toc\n",
    "    print \"BigramTagger on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime\n",
    "    \n",
    "    # Train and evaluate nb tagger on the \"news\" category (universal tagset)\n",
    "    nb_tagger = ClassifierBasedPOSTagger(train=train_sets_uni['news'])\n",
    "    \n",
    "    # Print the performance of the nb tagger and the runtime (universal tagset)\n",
    "    tic = time.time()\n",
    "    accuracy = nb_tagger.evaluate(test_sets_uni['news']) * 100\n",
    "    runtime_uni = round(time.time() - tic, 2)\n",
    "    print \"Naive Bayes on news, (universal tagset). Accuracy:\", round(accuracy,2), \"Evaluation runtime (s):\", runtime_uni\n",
    "    \n",
    "    ### Cross evaluation\n",
    "    \n",
    "    # Cross-evaluate between categories (using universal tagset)\n",
    "    # Example: train on news_train, evaluate on the \"test\" of every other category\n",
    "    # Do this for all categories in the corpus\n",
    "    # Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AffixTagger  BigramTagger  DefaultTagger  TrigramTagger  UnigramTagger\n",
      "adventure              23.71         90.17          10.87          90.05          88.79\n",
      "belles_lettres         28.15         90.38          12.86          90.34          89.11\n",
      "editorial              29.28         87.09          13.93          86.85          86.12\n",
      "fiction                24.46         88.74          11.38          88.69          87.44\n",
      "government             31.51         86.49          13.67          86.48          85.25\n",
      "hobbies                28.19         85.97          13.63          85.94          85.01\n",
      "humor                  24.44         83.24          11.73          83.33          82.82\n",
      "learned                34.94         90.24          18.04          90.31          89.19\n",
      "lore                   27.94         88.32          14.05          88.18          87.16\n",
      "mystery                23.33         89.47          11.30          89.25          87.53\n",
      "news                   29.60         88.31          14.05          88.31          87.54\n",
      "religion               27.70         87.89          13.13          87.99          87.08\n",
      "reviews                27.00         85.15          12.20          85.22          84.57\n",
      "romance                23.73         87.17          10.37          87.16          86.22\n",
      "science_fiction        19.11         80.51           8.01          80.74          79.77\n",
      "                 AffixTagger  BigramTagger  DefaultTagger  TrigramTagger  UnigramTagger\n",
      "adventure              28.72         94.14          17.91          93.96          93.27\n",
      "belles_lettres         34.24         94.09          22.31          93.96          93.59\n",
      "editorial              37.85         92.96          27.09          92.69          92.39\n",
      "fiction                30.93         93.37          19.79          93.23          92.91\n",
      "government             40.23         93.49          27.99          93.61          93.03\n",
      "hobbies                34.47         91.35          23.97          91.41          90.93\n",
      "humor                  32.77         90.97          22.04          90.68          90.84\n",
      "learned                39.74         94.46          27.05          94.52          93.73\n",
      "lore                   34.48         92.75          23.64          92.67          92.27\n",
      "mystery                29.16         93.56          17.97          93.50          92.66\n",
      "news                   41.00         94.52          30.62          94.25          94.13\n",
      "religion               33.64         92.22          21.15          92.22          92.02\n",
      "reviews                36.37         92.46          25.80          92.15          92.30\n",
      "romance                32.84         89.94          18.49          89.70          89.62\n",
      "science_fiction        27.72         88.97          17.40          89.01          89.12\n",
      "BigramTagger on news, (full tagset). Accuracy: 88.31 Evaluation runtime (s): 6345.68075514\n",
      "Naive Bayes on news, (full tagset). Accuracy: 89.71 Evaluation runtime (s): 98.72\n",
      "BigramTagger on news, (universal tagset). Accuracy: 94.52 Evaluation runtime (s): 6446.99461913\n",
      "Naive Bayes on news, (universal tagset). Accuracy: 91.94 Evaluation runtime (s): 5.43\n"
     ]
    }
   ],
   "source": [
    "hw2_part2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90.05 90.34 86.85 88.69 tri none\n",
    "#93.96 93.96 92.69 93.23 tri universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
